library(tidyverse) # plotting and manipulation
library(grid) # combining plots
library(gridExtra) # combining plots
library(ggpubr) # combining plots
library(patchwork) # combining plots
library(ggfortify) # nice extension for ggplot
library(mgcv) #fitting gam models
library(GGally) # displaying pairs panel
library(caTools) # split dataset
library(readxl)
library(randomForest)
library(e1071)
library(gbm)          # basic implementation
library(xgboost)      # a faster implementation of gbm
library(caret)        # an aggregator package for performing many machine learning models
library(pdp)          # model visualization
library(lime)         # model visualization
library(neuralnet)
library(rpart)     #rpart for computing decision tree models
library(rsample)     # data splitting 
library(dplyr)       # data wrangling
library(rpart.plot)  # plotting regression trees
library(ipred)       # bagging
library(broom)
library(ranger) 	#efficient RF
library(NeuralNetTools)
library(tidymodels)
library(earth) 		#MARS model
library(iml)		#most robust and efficient relative importance 
library(xgboost)	#extreeme gradient boosting
library(ModelMetrics) #get model metrics
library(Metrics) 	#get ML model metrics
library(Cubist) #Cubist model
library(FactoMineR)#PCANguyen
library(factoextra) #PCANguyen
library(rattle)
library(vip)
library(shapper)
library("DALEX2")
library(viridis)
library(tidytext)
library(fastshap)
library(shapviz)
library(readxl)
library(boot)

library(readxl)
N_all_fix <- read_excel("D:/Publikasi/Pak Heru B Pulunggono/16 Machine Learning for N in peat yusuf/data/R_fix_yusuf.xlsx")
View(N_all_fix)
str(N_all_fix)
		tibble [120 × 13] (S3: tbl_df/tbl/data.frame)
		 $ LU_OP   : num [1:120] 1 1 1 1 1 1 1 1 1 1 ...
		 $ LU_B    : num [1:120] 0 0 0 0 0 0 0 0 0 0 ...
		 $ OP_Dist1: num [1:120] 1 1 1 1 1 1 1 1 1 1 ...
		 $ OP_Dist2: num [1:120] 0 0 0 0 0 0 0 0 0 0 ...
		 $ OP_Dist3: num [1:120] 0 0 0 0 0 0 0 0 0 0 ...
		 $ OP_Dist4: num [1:120] 0 0 0 0 0 0 0 0 0 0 ...
		 $ Depth   : num [1:120] 20 40 60 20 40 60 20 40 60 20 ...
		 $ Org_C   : num [1:120] 0.38 0.417 0.487 0.546 0.555 ...
		 $ pH      : num [1:120] 3.99 3.77 3.72 3.99 3.84 3.78 3.92 3.85 3.73 3.63 ...
		 $ Total_N : num [1:120] 0.438 0.326 0.739 0.389 0.237 ...
		 $ PD      : num [1:120] 1.83 1.77 1.62 1.83 1.77 1.62 1.76 1.77 1.62 1.83 ...
		 $ BD      : num [1:120] 0.166 0.105 0.103 0.163 0.115 ...
		 $ Total_Fe: num [1:120] 325 219 182 675 183 ...

N_all_fix$Org_C <- N_all_fix$Org_C *100		#convert Org-C to percent value 
N_all_fix <- na.omit(N_all_fix)			#omit missing values

#other preprocessing had been done outside of R (resulting in N.A)
str(N_all_fix)
		tibble [109 × 13] (S3: tbl_df/tbl/data.frame)
		 $ LU_OP   : num [1:109] 1 1 1 1 1 1 1 1 1 1 ...
		 $ LU_B    : num [1:109] 0 0 0 0 0 0 0 0 0 0 ...
		 $ OP_Dist1: num [1:109] 1 1 1 1 1 1 1 1 1 1 ...
		 $ OP_Dist2: num [1:109] 0 0 0 0 0 0 0 0 0 0 ...
		 $ OP_Dist3: num [1:109] 0 0 0 0 0 0 0 0 0 0 ...
		 $ OP_Dist4: num [1:109] 0 0 0 0 0 0 0 0 0 0 ...
		 $ Depth   : num [1:109] 20 40 60 20 40 60 20 40 20 40 ...
		 $ Org_C   : num [1:109] 38 41.7 48.7 54.6 55.5 ...
		 $ pH      : num [1:109] 3.99 3.77 3.72 3.99 3.84 3.78 3.92 3.85 3.63 3.55 ...
		 $ Total_N : num [1:109] 0.438 0.326 0.739 0.389 0.237 ...
		 $ PD      : num [1:109] 1.83 1.77 1.62 1.83 1.77 1.62 1.76 1.77 1.83 1.77 ...
		 $ BD      : num [1:109] 0.166 0.105 0.103 0.163 0.115 ...
		 $ Total_Fe: num [1:109] 325 219 182 675 183 ...
		 - attr(*, "na.action")= 'omit' Named int [1:11] 9 21 40 43 49 55 56 57 58 93 ...
		  ..- attr(*, "names")= chr [1:11] "9" "21" "40" "43" ...


## bootstrapping with replacement 
boots_N <- N_all_fix %>% 
  bind_rows(
    N_all_fix %>% 
      sample_n(500,replace = TRUE) 
  )
  
str(boots_N)
		tibble [609 × 13] (S3: tbl_df/tbl/data.frame)
		 $ LU_OP   : num [1:609] 1 1 1 1 1 1 1 1 1 1 ...
		 $ LU_B    : num [1:609] 0 0 0 0 0 0 0 0 0 0 ...
		 $ OP_Dist1: num [1:609] 1 1 1 1 1 1 1 1 1 1 ...
		 $ OP_Dist2: num [1:609] 0 0 0 0 0 0 0 0 0 0 ...
		 $ OP_Dist3: num [1:609] 0 0 0 0 0 0 0 0 0 0 ...
		 $ OP_Dist4: num [1:609] 0 0 0 0 0 0 0 0 0 0 ...
		 $ Depth   : num [1:609] 20 40 60 20 40 60 20 40 20 40 ...
		 $ Org_C   : num [1:609] 38 41.7 48.7 54.6 55.5 ...
		 $ pH      : num [1:609] 3.99 3.77 3.72 3.99 3.84 3.78 3.92 3.85 3.63 3.55 ...
		 $ Total_N : num [1:609] 0.438 0.326 0.739 0.389 0.237 ...
		 $ PD      : num [1:609] 1.83 1.77 1.62 1.83 1.77 1.62 1.76 1.77 1.83 1.77 ...
		 $ BD      : num [1:609] 0.166 0.105 0.103 0.163 0.115 ...
		 $ Total_Fe: num [1:609] 325 219 182 675 183 ...
		 - attr(*, "na.action")= 'omit' Named int [1:11] 9 21 40 43 49 55 56 57 58 93 ...
		  ..- attr(*, "names")= chr [1:11] "9" "21" "40" "43" ...

windowsFonts(Palatino=windowsFont("Palatino Linotype")) #setup font type to comply the Journal's font

# Correlation using ggally package

N_all_fix_corr = subset(N_all_fix, select = -c(1:6)) #subset to eliminate one-hot-encoded factors
str(N_all_fix_corr)
		tibble [109 × 7] (S3: tbl_df/tbl/data.frame)
		 $ Depth   : num [1:109] 20 40 60 20 40 60 20 40 20 40 ...
		 $ Org_C   : num [1:109] 38 41.7 48.7 54.6 55.5 ...
		 $ pH      : num [1:109] 3.99 3.77 3.72 3.99 3.84 3.78 3.92 3.85 3.63 3.55 ...
		 $ Total_N : num [1:109] 0.438 0.326 0.739 0.389 0.237 ...
		 $ PD      : num [1:109] 1.83 1.77 1.62 1.83 1.77 1.62 1.76 1.77 1.83 1.77 ...
		 $ BD      : num [1:109] 0.166 0.105 0.103 0.163 0.115 ...
		 $ Total_Fe: num [1:109] 325 219 182 675 183 ...
		 - attr(*, "na.action")= 'omit' Named int [1:11] 9 21 40 43 49 55 56 57 58 93 ...
		  ..- attr(*, "names")= chr [1:11] "9" "21" "40" "43" ...

#build a custom function for more flexibility to adjust ggally

my_fn_corr <- function(data, mapping, method="p", use="pairwise", ...){

              # grab data
              x <- eval_data_col(data, mapping$x)
              y <- eval_data_col(data, mapping$y)

              # calculate correlation
              corr <- cor(x, y, method=method, use='complete.obs')

              # calculate colour based on correlation value
              # Here I have set a correlation of minus one to blue, 
              # zero to white, and one to red 
              # Change this to suit: possibly extend to add as an argument of `my_fn_corr`
              colFn <- colorRampPalette(c("dodgerblue", "darkseagreen1", "green4"), interpolate ='spline')
              fill <- colFn(100)[findInterval(corr, seq(-1, 1, length=100))]

              ggally_cor(data = data, mapping = mapping, ...) + 
                theme_void() +
                theme(panel.background = element_rect(fill=fill))
            }

#correlation using original dataset --> to appendix
p1_corr <- ggpairs(N_all_fix_corr, 
                   upper = list(continuous = my_fn_corr),
                   lower = list(continuous = "smooth"))  			
p1_corr+theme(text = element_text(size=11, family="Palatino"))			


#correlation using bootstrapped sampling --> to article body
N_all_boot_corr = subset(boots_N, select = -c(1:6))

p1_boot_corr <- ggpairs(N_all_boot_corr, 
                   upper = list(continuous = my_fn_corr),
                   lower = list(continuous = "smooth"))  			
p1_boot_corr+theme(text = element_text(size=11, family="Palatino"))	


## Modelling

#sample split (70:30)
set.seed(42)

Split_N_all <- sample.split(Y=boots_N$Total_N, SplitRatio=0.7)
train_N_all<- subset(x=boots_N, Split_N_all==TRUE)
test_N_all <- subset(x=boots_N, Split_N_all==FALSE)

x_train_N_all = subset(train_N_all, select = -Total_N) %>% as.matrix() 
y_train_N_all = train_N_all$Total_N

x_test_N_all = subset(test_N_all, select = -Total_N) %>% as.matrix() 
y_test_N_all = test_N_all$Total_N 

X_df_train <- data.frame(x_train_N_all)
x_df_test <- data.frame(x_test_N_all)
train_N_all_df <- data.frame(train_N_all)
test_N_all_df <-data.frame(test_N_all)

summary(train_N_all)
			 LU_OP             LU_B           OP_Dist1        OP_Dist2         OP_Dist3     
		 Min.   :0.0000   Min.   :0.0000   Min.   :0.000   Min.   :0.0000   Min.   :0.0000  
		 1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:1.000   1st Qu.:0.0000   1st Qu.:0.0000  
		 Median :1.0000   Median :0.0000   Median :1.000   Median :0.0000   Median :0.0000  
		 Mean   :0.7564   Mean   :0.2436   Mean   :2.136   Mean   :0.2227   Mean   :0.2668  
		 3rd Qu.:1.0000   3rd Qu.:0.0000   3rd Qu.:4.500   3rd Qu.:0.0000   3rd Qu.:1.0000  
		 Max.   :1.0000   Max.   :1.0000   Max.   :4.500   Max.   :1.0000   Max.   :1.0000  
			OP_Dist4          Depth           Org_C             pH           Total_N       
		 Min.   :0.0000   Min.   :20.00   Min.   :37.83   Min.   :3.440   Min.   :0.06157  
		 1st Qu.:0.0000   1st Qu.:20.00   1st Qu.:48.45   1st Qu.:3.630   1st Qu.:0.22879  
		 Median :0.0000   Median :40.00   Median :52.89   Median :3.720   Median :0.37781  
		 Mean   :0.2436   Mean   :38.75   Mean   :51.05   Mean   :3.713   Mean   :0.37495  
		 3rd Qu.:0.0000   3rd Qu.:60.00   3rd Qu.:55.42   3rd Qu.:3.810   3rd Qu.:0.49361  
		 Max.   :1.0000   Max.   :60.00   Max.   :57.00   Max.   :3.990   Max.   :0.78432  
			   PD              BD             Total_Fe     
		 Min.   :0.660   Min.   :0.05999   Min.   : 33.16  
		 1st Qu.:1.230   1st Qu.:0.09840   1st Qu.:182.59  
		 Median :1.620   Median :0.12000   Median :316.01  
		 Mean   :1.528   Mean   :0.12132   Mean   :373.67  
		 3rd Qu.:1.770   3rd Qu.:0.14542   3rd Qu.:526.88  
		 Max.   :2.000   Max.   :0.18205   Max.   :791.34  

summary(test_N_all )
			 LU_OP             LU_B           OP_Dist1       OP_Dist2         OP_Dist3     
		 Min.   :0.0000   Min.   :0.0000   Min.   :0.00   Min.   :0.0000   Min.   :0.0000  
		 1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:1.00   1st Qu.:0.0000   1st Qu.:0.0000  
		 Median :1.0000   Median :0.0000   Median :1.00   Median :0.0000   Median :0.0000  
		 Mean   :0.7584   Mean   :0.2416   Mean   :2.16   Mean   :0.2191   Mean   :0.2753  
		 3rd Qu.:1.0000   3rd Qu.:0.0000   3rd Qu.:4.50   3rd Qu.:0.0000   3rd Qu.:1.0000  
		 Max.   :1.0000   Max.   :1.0000   Max.   :4.50   Max.   :1.0000   Max.   :1.0000  
			OP_Dist4          Depth           Org_C             pH           Total_N       
		 Min.   :0.0000   Min.   :20.00   Min.   :37.83   Min.   :3.440   Min.   :0.06157  
		 1st Qu.:0.0000   1st Qu.:20.00   1st Qu.:48.45   1st Qu.:3.630   1st Qu.:0.21987  
		 Median :0.0000   Median :40.00   Median :53.74   Median :3.720   Median :0.37781  
		 Mean   :0.2416   Mean   :38.65   Mean   :51.22   Mean   :3.717   Mean   :0.37484  
		 3rd Qu.:0.0000   3rd Qu.:60.00   3rd Qu.:55.42   3rd Qu.:3.810   3rd Qu.:0.49520  
		 Max.   :1.0000   Max.   :60.00   Max.   :57.00   Max.   :3.990   Max.   :0.78432  
			   PD              BD             Total_Fe     
		 Min.   :0.660   Min.   :0.06000   Min.   : 33.16  
		 1st Qu.:1.280   1st Qu.:0.09975   1st Qu.:182.46  
		 Median :1.620   Median :0.12113   Median :312.96  
		 Mean   :1.518   Mean   :0.12350   Mean   :368.22  
		 3rd Qu.:1.770   3rd Qu.:0.14159   3rd Qu.:526.88  
		 Max.   :2.000   Max.   :0.18205   Max.   :791.34  


##visualize distribution 
N_train <- train_N_all%>%
  add_column(Status = "Training")

N_test <- test_N_all%>%
  add_column(Status = "Validation")

N_df <- bind_rows(N_train, N_test)

str(N_df)

windowsFonts(Palatino=windowsFont("Palatino Linotype"))

# Represent it
plot_dist_N <- N_df %>% 
				ggplot( aes(x=Total_N, color=Status, fill=Status)) +
				geom_density(alpha = 0.5, linewidth=0.8)+
				#geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity') +
				scale_fill_manual(values=c("#69b3a2", "#404080")) +
				scale_color_manual(values=c("#69b3a2", "#404080")) +
				theme_bw()+
				labs(x = " Total N (%)", y = "Frequency")+
				theme(text=element_text(size=13,family="Palatino"))
				
				
plot_dist_N


baru_N <-  N_df%>% subset(,-c(1:7))%>%
		pivot_longer(cols = -"Status", names_to="Property", values_to="value")

str(baru_N)	
		tibble [3,654 × 3] (S3: tbl_df/tbl/data.frame)
		 $ Status  : chr [1:3654] "Training" "Training" "Training" "Training" ...
		 $ Property: chr [1:3654] "Org_C" "pH" "Total_N" "PD" ...
		 $ value   : num [1:3654] 41.73 3.77 0.326 1.77 0.105 ...

IndvarNames1 = list(
'BD'="BD (g/cm3)",
'Org_C'="Org-C (%)",
'PD'="PD (g/cm3)",
'pH'="pH",
'Total_N'="Total N (%)",
'Total_Fe' = "Total Fe (mg/kg")

#bikin fungsi label, passing ke facet_wrap ggplot2

indvar_labeller1 <- function(variable,value){
  return(IndvarNames1[value])
}

# fungsi eliminasi outlier, passing ke stat_summary ggplot2

calc_stat1 <- function(x) {
  coef <- 1.5
  n <- sum(!is.na(x))
  # calculate quantiles
  stats <- quantile(x, probs = c(0.1, 0.25, 0.5, 0.75, 0.9))
  names(stats) <- c("ymin", "lower", "middle", "upper", "ymax")
  return(stats)
}

boxplot_N <- baru_N %>%
				ggplot(aes(x=Property, y=value, fill=Status)) +
				stat_summary(aes(fill=factor(Status)), fun.data = calc_stat1, geom="boxplot" ,
								position=position_dodge(width=1.1),alpha = 0.7 ) + 
				facet_wrap(~Property, labeller=indvar_labeller1, scales="free")+
				scale_fill_manual(values=c("#69b3a2", "#404080")) +
				theme_bw()+
				theme(text=element_text(size=13, family="Palatino"),
				axis.text.x=element_blank(),
				axis.title.x=element_blank(),
				axis.title.y=element_blank())

boxplot_N

plot_dist_N+boxplot_N+plot_layout(ncol=1) #merge total N distribution to covariate's boxplots


## modeling on the go

set.seed = 42
MLR_N_All <- train(
  x 			= x_train_N_all,
  y 			= y_train_N_all,
  method = "lm",
  family = "gaussian",
  metric = "RMSE",
  trControl = trainControl(method = "repeatedcv", number = 5, repeats=10)
)

MLR_N_All 
		Linear Regression 

		431 samples
		 12 predictor

		No pre-processing
		Resampling: Cross-Validated (5 fold, repeated 10 times) 
		Summary of sample sizes: 346, 345, 344, 344, 345, 345, ... 
		Resampling results:

		  RMSE       Rsquared   MAE      
		  0.1475752  0.2687833  0.1203446

		Tuning parameter 'intercept' was held constant at a value of TRUE


preds_MLR_N_All <- predict(MLR_N_All, test_N_all)

modelEval_MLR_N_All <- cbind(test_N_all$Total_N, preds_MLR_N_All)
colnames(modelEval_MLR_N_All) <- c('Actual', 'Predicted')
modelEval_MLR_N_All <- as.data.frame(modelEval_MLR_N_All)

mse_MLR_N_All <- mean((modelEval_MLR_N_All$Actual - modelEval_MLR_N_All$Predicted)^2)
rmse_MLR_N_All <- sqrt(mse_MLR_N_All)
rmse_MLR_N_All
[1]  0.1401671

mae_MLR_N_All <- ModelMetrics::mae(y_test_N_all, preds_MLR_N_All)
mae_MLR_N_All
[1] 0.1148419

bias_MLR_N_All <- Metrics::bias(y_test_N_all, preds_MLR_N_All)
bias_MLR_N_All
[1] 9.123912e-05



## Log - GLM regression

tuned_log_N_All <- train(
  x 			= x_train_N_all,
  y 			= y_train_N_all,
  method = "glm",
  family = "binomial",
  metric = "RMSE",
  trControl = trainControl(method = "repeatedcv", number = 5, repeats=10)
)

tuned_log_N_All
		Generalized Linear Model 

		431 samples
		 12 predictor

		No pre-processing
		Resampling: Cross-Validated (5 fold, repeated 10 times) 
		Summary of sample sizes: 345, 345, 344, 345, 345, 344, ... 
		Resampling results:

		  RMSE       Rsquared   MAE      
		  0.1470837  0.2702196  0.1200248

preds_LOG_GLM_N_All <- predict(tuned_log_N_All, test_N_all)

modelEval_LOG_GLM_N_All <- cbind(test_N_all$Total_N, preds_LOG_GLM_N_All)
colnames(modelEval_LOG_GLM_N_All) <- c('Actual', 'Predicted')
modelEval_LOG_GLM_N_All <- as.data.frame(modelEval_LOG_GLM_N_All)

mse_LOG_GLM_N_All <- mean((modelEval_LOG_GLM_N_All$Actual - modelEval_LOG_GLM_N_All$Predicted)^2)
rmse_LOG_GLM_N_All <- sqrt(mse_LOG_GLM_N_All)
rmse_LOG_GLM_N_All
[1] 0.1393914

mae_log_GLM_N_All <- ModelMetrics::mae(y_test_N_all, preds_LOG_GLM_N_All)
mae_log_GLM_N_All
[1] 0.1141461

bias_log_GLM_N_All <- Metrics::bias(y_test_N_all, preds_LOG_GLM_N_All)
bias_log_GLM_N_All
[1] -0.0001055489


# Multivariate adaptive regression spline/MARS 

#tuning model 1
hyper_grid_mars <- expand.grid(
  degree = 1:5, 
  nprune = seq(2, 100, length.out = 10) %>% floor()
)

set.seed(42)
# cross validated model
tuned_mars_N_All <- train(
  x 			= X_df_train,
  y 			= y_train_N_all,
  method 		= "earth",
  metric 		= "RMSE",
  trControl 	= trainControl(method = "repeatedcv", number = 5, repeats = 10),
  tuneGrid 		= hyper_grid_mars
							)
summary(tuned_mars_N_All)
			Call: earth(x=data.frame[431,12], y=c(0.3257,0.3886...), keepxy=TRUE,
						degree=3, nprune=23)

														   coefficients
			(Intercept)                                     0.149855370
			h(40-Depth)                                    -0.006020791
			h(Depth-40)                                     0.014085582
			h(47.0239-Org_C)                                0.028222459
			h(Org_C-47.0239)                                0.018038780
			h(215.104-Total_Fe)                             0.004457609
			h(Total_Fe-215.104)                             0.000743046
			h(OP_Dist1-3) * h(47.0239-Org_C)                0.063755432
			h(Depth-40) * h(52.8884-Org_C)                  0.002647498
			h(40-Depth) * h(BD-0.12)                        0.132317542
			h(40-Depth) * h(0.12-BD)                        0.210493220
			h(Depth-40) * h(Total_Fe-179.072)              -0.000046731
			h(Depth-40) * h(179.072-Total_Fe)              -0.000409814
			h(Org_C-49.1428) * h(215.104-Total_Fe)         -0.000506622
			h(3.55-pH) * h(215.104-Total_Fe)               -0.058099200
			h(pH-3.55) * h(215.104-Total_Fe)               -0.013651492
			h(0.131362-BD) * h(Total_Fe-215.104)           -0.016623239
			h(BD-0.131362) * h(Total_Fe-215.104)           -0.015885566
			h(Depth-40) * h(pH-3.59) * h(179.072-Total_Fe)  0.003053125
			h(40-Depth) * h(BD-0.12) * h(325.382-Total_Fe) -0.002192144

			Selected 20 of 25 terms, and 6 of 12 predictors (nprune=23)
			Termination condition: Reached nk 25
			Importance: Depth, Org_C, Total_Fe, OP_Dist1, BD, pH, LU_OP-unused, ...
			Number of terms at each degree of interaction: 1 6 11 2
			GCV 0.01053951    RSS 3.577716    GRSq 0.6436777    RSq 0.7180521

tuned_mars_N_All$bestTune
		   nprune degree
		23     23      3


hyper_grid_mars2 <- expand.grid(
  degree = 3:5, 
  nprune = seq(20, 30, length.out = 2) %>% floor()
)

#tuning model 2
set.seed(42)
# cross validated model
tuned_mars_N_All2 <- train(
  x 			= X_df_train,
  y 			= y_train_N_all,
  method 		= "earth",
  metric 		= "RMSE",
  trControl 	= trainControl(method = "repeatedcv", number = 5, repeats = 10),
  tuneGrid 		= hyper_grid_mars2
							)
summary(tuned_mars_N_All2)
		Call: earth(x=data.frame[431,12],
					y=c(0.3257,0.3886...),
					keepxy=TRUE, degree=3,
					nprune=30)

													   coefficients
		(Intercept)                                     0.149855370
		h(40-Depth)                                    -0.006020791
		h(Depth-40)                                     0.014085582
		h(47.0239-Org_C)                                0.028222459
		h(Org_C-47.0239)                                0.018038780
		h(215.104-Total_Fe)                             0.004457609
		h(Total_Fe-215.104)                             0.000743046
		h(OP_Dist1-3) * h(47.0239-Org_C)                0.063755432
		h(Depth-40) * h(52.8884-Org_C)                  0.002647498
		h(40-Depth) * h(BD-0.12)                        0.132317542
		h(40-Depth) * h(0.12-BD)                        0.210493220
		h(Depth-40) * h(Total_Fe-179.072)              -0.000046731
		h(Depth-40) * h(179.072-Total_Fe)              -0.000409814
		h(Org_C-49.1428) * h(215.104-Total_Fe)         -0.000506622
		h(3.55-pH) * h(215.104-Total_Fe)               -0.058099200
		h(pH-3.55) * h(215.104-Total_Fe)               -0.013651492
		h(0.131362-BD) * h(Total_Fe-215.104)           -0.016623239
		h(BD-0.131362) * h(Total_Fe-215.104)           -0.015885566
		h(Depth-40) * h(pH-3.59) * h(179.072-Total_Fe)  0.003053125
		h(40-Depth) * h(BD-0.12) * h(325.382-Total_Fe) -0.002192144

		Selected 20 of 25 terms, and 6 of 12 predictors (nprune=30)
		Termination condition: Reached nk 25
		Importance: Depth, Org_C, Total_Fe, ...
		Number of terms at each degree of interaction: 1 6 11 2
		GCV 0.01053951    RSS 3.577716    GRSq 0.6436777    RSq 0.7180521


tuned_mars_N_All2$bestTune
  nprune degree
1     20      3

#resamples to get model comparison
res_mars <- resamples(list(mars1 = tuned_mars_N_All, mars2 = tuned_mars_N_All2))
		summary(res_mars)

		Call:
		summary.resamples(object = res_mars)

		Models: mars1, mars2 
		Number of resamples: 50 

		MAE 
					Min.    1st Qu.     Median       Mean    3rd Qu.       Max. NAs
		mars1 0.04889241 0.06747015 0.07528598 0.07453583 0.08156179 0.09728019    0
		mars2 0.04889241 0.06747015 0.07528598 0.07453583 0.08156179 0.09728019    0

		RMSE 
					Min.    1st Qu.    Median      Mean   3rd Qu.      Max. NAs
		mars1 0.06865035 0.09453587 0.1004607 0.1046101 0.1150685 0.1396338    0
		mars2 0.06865035 0.09453587 0.1004607 0.1046101 0.1150685 0.1396338    0

		Rsquared 
				   Min.   1st Qu.    Median      Mean   3rd Qu.    Max. NAs
		mars1 0.4255194 0.5683839 0.6454163 0.6290887 0.6893031 0.83988    0
		mars2 0.4255194 0.5683839 0.6454163 0.6290887 0.6893031 0.83988    0	
		
		
scales_mars <- list(x=list(relation="free"), y=list(relation="free"))

bwplot(res_mars, scales=scales_mars)

ggplot(res_mars, scales=scales_mars)

res_mars$values %>% #extract the values
  select(1, ends_with("Rsquared")) %>% #select the first column and all columns with a name ending with "Rsquared"
  gather(model, Rsquared, -1) %>% #convert to long table
  mutate(model = sub("~Rsquared", "", model)) %>% #leave just the model names
  ggplot()+ #call ggplot
  geom_boxplot(aes(x = Rsquared, y = model, fill=model)) -> plotres_mars #and plot the box plot

plot_res_mars<- plotres_mars + scale_y_discrete(limits = c("mars1", "mars2"))+
    scale_fill_viridis(discrete = TRUE) + theme_bw()  + 
	theme(text=element_text(size=13, family="Palatino"),
	axis.title.y=element_blank())+theme( legend.position="none") 

plot_res_mars #similar performance (both in resamples summary and boxplots)


preds_MARS_N_All <- predict(tuned_mars_N_All, test_N_all)

modelEval_MARS_N_All <- cbind(test_N_all$Total_N, preds_MARS_N_All)
colnames(modelEval_MARS_N_All) <- c('Actual', 'Predicted')
modelEval_MARS_N_All <- as.data.frame(modelEval_MARS_N_All)

mse_MARS_N_All <- mean((modelEval_MARS_N_All$Actual - modelEval_MARS_N_All$Predicted)^2)
rmse_MARS_N_All <- sqrt(mse_MARS_N_All)
rmse_MARS_N_All
	[1] 0.09123466

mae_mars_N_All <- ModelMetrics::mae(y_test_N_all, preds_MARS_N_All)
mae_mars_N_All
	[1] 0.06408076

bias_mars_N_All <- Metrics::bias(y_test_N_all, preds_MARS_N_All)
bias_mars_N_All
	[1] -0.004937763


#Cubist (Quinlan's rule-based)

#tuning1
hyper_grid_cubist1 <- expand.grid(committees = c(1, 10, 50, 100), neighbors = c(0, 1, 5, 9))

set.seed(42)
# cross validated model
tuned_cubist_N_All1 <- train(
    x 			= X_df_train,
  y 			= y_train_N_all,
  method = "cubist",
  tuneGrid = hyper_grid_cubist1,
  trControl = trainControl(method = "repeatedcv", number = 5, repeats = 10)
  )
tuned_cubist_N_All1
		Cubist 

		431 samples
		 12 predictor

		No pre-processing
		Resampling: Cross-Validated (5 fold, repeated 10 times) 
		Summary of sample sizes: 344, 345, 345, 345, 345, 345, ... 
		Resampling results across tuning parameters:

		  committees  neighbors  RMSE        Rsquared   MAE        
			1         0          0.05150382  0.8891711  0.012324402
			1         1          0.03421649  0.9331255  0.006429279
			1         5          0.04385089  0.9146145  0.010912761
			1         9          0.04678617  0.9059222  0.012063629
		   10         0          0.03440519  0.9387885  0.008518483
		   10         1          0.02787498  0.9497579  0.005434174
		   10         5          0.03149450  0.9449687  0.007702649
		   10         9          0.03262233  0.9428256  0.008295081
		   50         0          0.03361654  0.9401451  0.008246351
		   50         1          0.02802358  0.9497191  0.005484921
		   50         5          0.03128395  0.9455304  0.007508155
		   50         9          0.03215416  0.9437256  0.008010001
		  100         0          0.03164929  0.9458465  0.007699530
		  100         1          0.02691347  0.9533285  0.005239690
		  100         5          0.02959422  0.9501918  0.007021041
		  100         9          0.03035295  0.9487890  0.007481520

		RMSE was used to select the optimal model using the smallest value.
		The final values used for the model were committees = 100 and neighbors = 1.

#tuning2
hyper_grid_cubist2 <- expand.grid(committees = c(50, 100), neighbors = c(1, 2, 3, 4))

set.seed(42)
# cross validated model
tuned_cubist_N_All2 <- train(
    x 			= X_df_train,
  y 			= y_train_N_all,
  method = "cubist",
  tuneGrid = hyper_grid_cubist2,
  trControl = trainControl(method = "repeatedcv", number = 5, repeats = 10)
  )
tuned_cubist_N_All2



res_cubist <- resamples(list(cubist1 = tuned_cubist_N_All1, cubist2 = tuned_cubist_N_All2))
			summary(res_cubist)

		Call:
		summary.resamples(object = res_cubist)

		Models: cubist1, cubist2 
		Number of resamples: 50 

		MAE 
						Min.      1st Qu.      Median       Mean     3rd Qu.      Max. NAs
		cubist1 1.294117e-08 0.0008662386 0.002899379 0.00523969 0.008153754 0.0300755    0
		cubist2 1.294117e-08 0.0008662386 0.002899379 0.00523969 0.008153754 0.0300755    0

		RMSE 
						Min.     1st Qu.    Median       Mean    3rd Qu.      Max. NAs
		cubist1 3.352209e-08 0.005935388 0.0172291 0.02691347 0.03662521 0.1234791    0
		cubist2 3.352209e-08 0.005935388 0.0172291 0.02691347 0.03662521 0.1234791    0

		Rsquared 
					 Min.  1st Qu.   Median      Mean  3rd Qu. Max. NAs
		cubist1 0.5563984 0.950178 0.991041 0.9533285 0.998797    1    0
		cubist2 0.5563984 0.950178 0.991041 0.9533285 0.998797    1    0
		
		
scales_cubist <- list(x=list(relation="free"), y=list(relation="free"))

bwplot(res_cubist, scales=scales_cubist)

ggplot(res_cubist, scales=scales_cubist)

res_cubist$values %>% #extract the values
  select(1, ends_with("Rsquared")) %>% #select the first column and all columns with a name ending with "Rsquared"
  gather(model, Rsquared, -1) %>% #convert to long table
  mutate(model = sub("~Rsquared", "", model)) %>% #leave just the model names
  ggplot()+ #call ggplot
  geom_boxplot(aes(x = Rsquared, y = model, fill=model)) -> plotres_cubist #and plot the box plot

plot_res_cubist<- plotres_cubist + scale_y_discrete(limits = c("cubist1", "cubist2"))+
    scale_fill_viridis(discrete = TRUE) + theme_bw()  + 
	theme(text=element_text(size=13, family="Palatino"),
	axis.title.y=element_blank())+theme( legend.position="none") ## kesimpulan:

plot_res_cubist #podo

preds_CUBIST_N_All <- predict(tuned_cubist_N_All1, test_N_all)

modelEval_CUBIST_N_All <- cbind(test_N_all$Total_N, preds_CUBIST_N_All)
colnames(modelEval_CUBIST_N_All) <- c('Actual', 'Predicted')
modelEval_CUBIST_N_All <- as.data.frame(modelEval_CUBIST_N_All)

mse_CUBIST_N_All <- mean((modelEval_CUBIST_N_All$Actual - modelEval_CUBIST_N_All$Predicted)^2)
rmse_CUBIST_N_All <- sqrt(mse_CUBIST_N_All)
rmse_CUBIST_N_All
1.213287e-07

mae_Cubist_N_All <- ModelMetrics::mae(y_test_N_all, preds_CUBIST_N_All)
mae_Cubist_N_All
2.328173e-08

bias_Cubist_N_All <- Metrics::bias(y_test_N_all, preds_CUBIST_N_All)
bias_Cubist_N_All
707359e-09


## Random Forest

#try ranger first to grasp initial hyperparameterisation

hyper_grid_RF_N_all <- expand.grid(
  mtry       = seq(2, 10, by = 1),
  node_size  = seq(20, 30, by = 1),
  num.trees	 = seq(50, 1000, by = 50),
  OOB_RMSE   = 0
)

for(i in 1:nrow(hyper_grid_RF_N_all)) {
  
  # train model
  model_rf_N_all <- ranger(
    formula         = Total_N ~., 
    data            = boots_N, 
    num.trees       = hyper_grid_RF_N_all$num.trees[i],
    mtry            = hyper_grid_RF_N_all$mtry[i],
    min.node.size   = hyper_grid_RF_N_all$node_size[i],
    sample.fraction = .70,
    seed            = 42
  )
  
  # add OOB error to grid
  hyper_grid_RF_N_all$OOB_RMSE[i] <- sqrt(model_rf_N_all$prediction.error)
}

hyper_grid_RF_N_all %>% 
  dplyr::arrange(OOB_RMSE) %>%  
  arrange(OOB_RMSE) %>%
		  top_n(-10, wt = OOB_RMSE)
		   mtry node_size num.trees   OOB_RMSE
		1     9        20       900 0.05212114
		2     9        20       700 0.05220565
		3     9        20       750 0.05221511
		4     9        20       600 0.05224545
		5     9        20       500 0.05228585
		6     9        20       550 0.05229629
		7     9        20       650 0.05229720
		8     9        20       800 0.05230081
		9     9        20       950 0.05232084
		10    9        20       850 0.05236165



# use aforementioned tuning parameterisation to supply caret

# tuning model 1
tunegrid_N1 <- expand.grid(.mtry = c(8:11)) 

set.seed(123)
RF_N_caret <- train(
			x = x_train_N_all,
			y = y_train_N_all,
                   method = 'rf',
                   metric = 'RMSE',
                   tuneGrid = tunegrid_N1, 
				   nodesize = 20,
					ntree = 900,
                   trControl = trainControl(method = "repeatedcv", number = 5, repeats=10))

print(RF_N_caret) 
		Random Forest 

		431 samples
		 12 predictor

		No pre-processing
		Resampling: Cross-Validated (5 fold, repeated 10 times) 
		Summary of sample sizes: 345, 345, 344, 345, 345, 344, ... 
		Resampling results across tuning parameters:

		  mtry  RMSE        Rsquared   MAE       
		   8    0.06985881  0.8559368  0.04789048
		   9    0.06949726  0.8546894  0.04721392
		  10    0.06929977  0.8531088  0.04673801
		  11    0.06925170  0.8512394  0.04637856

		RMSE was used to select the optimal model using the smallest value.
		The final value used for the model was mtry = 11.


# tuning model 2

tunegrid_N2 <- expand.grid(.mtry = c(8:11)) 

RF_N_caret2 <- train(
					x = x_train_N_all,
					y = y_train_N_all,
					method = "rf", 
					trControl = trainControl(method = "repeatedcv", number = 5, repeats=10), 
					metric= "RMSE",
					verbose = FALSE, 
					tuneGrid = tunegrid_N2,
					n.trees = c(4:50)*100) ## tweaking num.of.trees inside
 
print(RF_N_caret2) 
		Random Forest 

		431 samples
		 12 predictor

		No pre-processing
		Resampling: Cross-Validated (5 fold, repeated 10 times) 
		Summary of sample sizes: 345, 344, 345, 345, 345, 345, ... 
		Resampling results across tuning parameters:

		  mtry  RMSE        Rsquared   MAE       
		   8    0.03758451  0.9539520  0.02021405
		   9    0.03749239  0.9538281  0.02000775
		  10    0.03699880  0.9546418  0.01970626
		  11    0.03695258  0.9544924  0.01959157

		RMSE was used to select the optimal model using the smallest value.
		The final value used for the model was mtry = 11.

RF_N_caret2$finalModel$ntree
[1] 500

# RF model comparison
res_RF <- resamples(list(RF1 = RF_N_caret, RF2 = RF_N_caret2))
summary(res_RF)
	
	Call:
	summary.resamples(object = res_RF)
	
	Models: RF1, RF2 
	Number of resamples: 50 
	
	MAE 
	          Min.    1st Qu.     Median       Mean    3rd Qu.       Max. NA's
	RF1 0.03475104 0.04339117 0.04619703 0.04637856 0.04927786 0.05942593    0
	RF2 0.01229066 0.01676799 0.01879054 0.01959157 0.02150510 0.03763588    0
	
	RMSE 
	          Min.    1st Qu.     Median       Mean    3rd Qu.       Max. NA's
	RF1 0.04723525 0.06463236 0.06782356 0.06925170 0.07289575 0.10900853    0
	RF2 0.01809156 0.02986096 0.03491292 0.03695258 0.04241209 0.08535481    0
	
	Rsquared 
	         Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's
	RF1 0.5842640 0.8272512 0.8624608 0.8512394 0.8835827 0.9351897    0
	RF2 0.7576628 0.9472108 0.9675881 0.9544924 0.9756922 0.9908121    0 #tweaked model win 

scales_RF <- list(x=list(relation="free"), y=list(relation="free"))

bwplot(res_RF, scales=scales_RF)

ggplot(res_RF, scales=scales_RF)

res_RF$values %>% #extract the values
  select(1, ends_with("Rsquared")) %>% #select the first column and all columns with a name ending with "Rsquared"
  gather(model, Rsquared, -1) %>% #convert to long table
  mutate(model = sub("~Rsquared", "", model)) %>% #leave just the model names
  ggplot()+ #call ggplot
  geom_boxplot(aes(x = Rsquared, y = model, fill=model)) -> plotres_RF #and plot the box plot

plot_res_RF<- plotres_RF + scale_y_discrete(limits = c("RF1", "RF2"))+
    scale_fill_viridis(discrete = TRUE) + theme_bw()  + 
	theme(text=element_text(size=13, family="Palatino"),
	axis.title.y=element_blank())+theme( legend.position="none") 

plot_res_RF ## final to model 2


preds_RF_N_All_Final <- predict(RF_N_caret2, test_N_all)

modelEval_RF_N_All <- cbind(test_N_all$Total_N, preds_RF_N_All_Final)
colnames(modelEval_RF_N_All) <- c('Actual', 'Predicted')
modelEval_RF_N_All <- as.data.frame(modelEval_RF_N_All)

mse_RF_N_All <- mean((modelEval_RF_N_All$Actual - modelEval_RF_N_All$Predicted)^2)
rmse_RF_N_All <- sqrt(mse_RF_N_All)
rmse_RF_N_All
[1] 0.01681139

mae_rf_N_All <- ModelMetrics::mae(y_test_N_all, preds_RF_N_All_Final)
mae_rf_N_All
[1] 0.007871883

bias_rf_N_All <- Metrics::bias(y_test_N_all, preds_RF_N_All_Final)
bias_rf_N_All
[1] 0.001398181



## Gradient Boosting Machine

#fixing minobsinnode
grid_gbm <-expand.grid(n.trees = seq(50, 1000, by = 50),
			interaction.depth = c(1, 3, 5), 
			shrinkage = c(.01, .1, .3, .5),
			n.minobsinnode = c(3, 5, 7))
			

set.seed(1234)
gbm_N_All_caret1 <- train(
	x 		= x_train_N_all,
	y 		= y_train_N_all,
	method 	= "gbm",
	metric 	= "RMSE",
	trControl = trainControl(method = "repeatedcv", number = 5, repeats=10),
	tuneGrid = grid_gbm,
	verbose = FALSE
)

gbm_N_All_caret1$bestTune
			n.trees interaction.depth
		320    1000                 5
			shrinkage n.minobsinnode
		320       0.1              3

#evaluate
ggplot(gbm_N_All_caret1) 
# RMSE is stabilised in higher shrinkage+tress (>250), 
# seemingly no difference in varying int.depth and n.minobsinnode

#try adding more trees, eliminate low shrinkage
grid_gbm2 <-expand.grid(n.trees = seq(500, 5000, by = 500),
			interaction.depth = c(3, 5, 7), 
			shrinkage = c(0.3, 0.5),
			n.minobsinnode = c(3,5))
			

set.seed(1234)
gbm_N_All_caret2 <- train(
	x 		= x_train_N_all,
	y 		= y_train_N_all,
	method 	= "gbm",
	metric 	= "RMSE",
	trControl = trainControl(method = "repeatedcv", number = 5, repeats=10),
	tuneGrid = grid_gbm2,
	verbose = FALSE
)

gbm_N_All_caret2$bestTune
   n.trees interaction.depth
10    5000                 3
   shrinkage n.minobsinnode
10       0.3              3

#evaluate
ggplot(gbm_N_All_caret2)
# RMSE is significantly lower at shrinkage = 0.3, n.minobsinnode = 3, and int.depth = 3
# RMSE is stabilised at tree >1000


grid_gbm3 <-expand.grid(n.trees = seq(1000, 5000, by = 1000),
			interaction.depth = 3, 
			shrinkage = 0.3,
			n.minobsinnode = 3)
			

set.seed(1234)
gbm_N_All_caret3 <- train(
	x 		= x_train_N_all,
	y 		= y_train_N_all,
	method 	= "gbm",
	metric 	= "RMSE",
	trControl = trainControl(method = "repeatedcv", number = 5, repeats=10),
	tuneGrid = grid_gbm3,
	verbose = FALSE
)

gbm_N_All_caret3$bestTune
			  n.trees interaction.depth
			5    5000                 3
			  shrinkage n.minobsinnode
			5       0.3              3

#evaluate
ggplot(gbm_N_All_caret3)
# cutoff ntrees for RMSE = 2000

#try adding more trees (considering our high computational capability)
grid_gbm4 <-expand.grid(n.trees = seq(2000, 10000, by = 2000),
			interaction.depth = 3, 
			shrinkage = 0.3,
			n.minobsinnode = 3)
			

set.seed(1234)
gbm_N_All_caret4 <- train(
	x 		= x_train_N_all,
	y 		= y_train_N_all,
	method 	= "gbm",
	metric 	= "RMSE",
	trControl = trainControl(method = "repeatedcv", number = 5, repeats=10),
	tuneGrid = grid_gbm4,
	verbose = FALSE
)

gbm_N_All_caret4$bestTune
  n.trees interaction.depth
3    6000                 3
  shrinkage n.minobsinnode
3       0.3              3

#evaluate
ggplot(gbm_N_All_caret4)
# new cutoff ntrees for RMSE = 4000,
# adding more tress than 6000 did not significantly lower RMSE


#try to adjust ntrees
grid_gbm5 <-expand.grid(n.trees = seq(4000, 8000, by = 1000),
			interaction.depth = 3, 
			shrinkage = 0.3,
			n.minobsinnode = 3)
			

set.seed(1234)
gbm_N_All_caret5 <- train(
	x 		= x_train_N_all,
	y 		= y_train_N_all,
	method 	= "gbm",
	metric 	= "RMSE",
	trControl = trainControl(method = "repeatedcv", number = 5, repeats=10),
	tuneGrid = grid_gbm5,
	verbose = FALSE
)

gbm_N_All_caret5$bestTune
		  n.trees interaction.depth
		3    6000                 3
		  shrinkage n.minobsinnode
		3       0.3              3

#evaluate
ggplot(gbm_N_All_caret5)
# new cutoff ntrees for RMSE = 5000,
# adding more tress than 5000 did not significantly lower RMSE


res_GBM <- resamples(list(GBM1 = gbm_N_All_caret1, GBM2 = gbm_N_All_caret2, GBM3 = gbm_N_All_caret3,GBM4 = gbm_N_All_caret4,GBM5 = gbm_N_All_caret5)) #, GBM3 = gbm_N_All_caret3, GBM4 = gbm_N_All_caret4))
	
scales2 <- list(x=list(relation="free"), y=list(relation="free"))
bwplot(res_GBM, scales=scales2)

ggplot(res_GBM, scales=scales2)

res_GBM$values %>% #extract the values
  select(1, ends_with("Rsquared")) %>% #select the first column and all columns with a name ending with "Rsquared"
  gather(model, Rsquared, -1) %>% #convert to long table
  mutate(model = sub("~Rsquared", "", model)) %>% #leave just the model names
  ggplot()+ #call ggplot
  geom_boxplot(aes(x = Rsquared, y = model, fill=model)) -> plotres_GBM #and plot the box plot

plot_res_GBM <- plotres_GBM + scale_y_discrete(limits = c("GBM1", "GBM2", "GBM3", "GBM4", "GBM5"))+ ##,"GBM3", "GBM4"))+
    scale_fill_viridis(discrete = TRUE) + theme_bw()  + 
	theme(text=element_text(size=13, family="Palatino"),
	axis.title.y=element_blank())+theme( legend.position="none") ##GBM2 unstable, high deviation, no significant differences GBM3-5

plot_res_GBM

#Final Model GBM

grid_gbm_final <-expand.grid(n.trees = seq(4000, 8000, by = 2000),
			interaction.depth = 3, 
			shrinkage = 0.3,
			n.minobsinnode = 3)
			

set.seed(1234)
gbm_N_All_caret_final <- train(
	x 		= x_train_N_all,
	y 		= y_train_N_all,
	method 	= "gbm",
	metric 	= "RMSE",
	trControl = trainControl(method = "repeatedcv", number = 5, repeats=10),
	tuneGrid = grid_gbm_final,
	verbose = FALSE
)

gbm_N_All_caret_final$bestTune


preds_GBM_N_All_caret1 <- predict(gbm_N_All_caret_final, test_N_all)

modelEval_GBM_N_All_caret1 <- cbind(test_N_all$Total_N, preds_GBM_N_All_caret1)
colnames(modelEval_GBM_N_All_caret1) <- c('Actual', 'Predicted')
modelEval_GBM_N_All_caret1 <- as.data.frame(modelEval_GBM_N_All_caret1)

mse_GBM_N_All_caret1 <- mean((modelEval_GBM_N_All_caret1$Actual - modelEval_GBM_N_All_caret1$Predicted)^2)
rmse_GBM_N_All_caret1 <- sqrt(mse_GBM_N_All_caret1)
rmse_GBM_N_All_caret1
7.074973e-17

mae_gbm_N_All <- ModelMetrics::mae(y_test_N_all, preds_GBM_N_All_caret1)
mae_gbm_N_All
4.541467e-17

bias_gbm_N_All <- Metrics::bias(y_test_N_all, preds_GBM_N_All_caret1)
bias_gbm_N_All
-4.794854e-18



## extreeme gradient boosting


# Fixing nround, learning rate (eta), max tree depth
xgb_trc_N_All = trainControl(
  method = "repeatedcv",
  number = 5,  
  #repeats = 10, ## dihilangkan buat menambah kecepatan
  allowParallel = TRUE,
  verboseIter = FALSE,
  returnData = FALSE
)


nrounds <- 1000
xgbGrid_N_All <- expand.grid(
  nrounds = seq(from = 200, to = nrounds, by = 50),
  eta = c(0.025, 0.05, 0.1, 0.3),
  max_depth = c(2, 3, 4, 5, 6),
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)


set.seed(42) 
xgb_N_All = train(
	x 		= x_train_N_all,
	y 		= y_train_N_all, 
  trControl = xgb_trc_N_All,
  tuneGrid = xgbGrid_N_All,
  method = "xgbTree",
  metric = 'RMSE',
  verbosity = 0
)

xgb_N_All$bestTune
    nrounds max_depth eta gamma
208     350         4 0.1     0
    colsample_bytree
208                1
    min_child_weight subsample
208                1         1

#evaluate
ggplot(xgb_N_All )
# eta = 0.3 scored significantly lower RMSE, stabilised at all parameterisations (<0.03), eventhough setting eta=0.1 resulted in lowest RMSE
# max_depth>4 yielded lower RMSE
#nrounds > 500 did not signficantly lower RMSE


# re-evaluate eta,max_depth, and nrounds,
nrounds <- 600
xgbGrid_N_All2 <- expand.grid(
  nrounds = seq(from = 300, to = nrounds, by = 50),
  eta = c(0.1, 0.3),
  max_depth = c(4, 5, 6),
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

set.seed(42) 
xgb_N_All2 = train(
	x 		= x_train_N_all,
	y 		= y_train_N_all, 
  trControl = xgb_trc_N_All,
  tuneGrid = xgbGrid_N_All2,
  method = "xgbTree",
  metric = 'RMSE',
  verbosity = 0
)

xgb_N_All2$bestTune
  nrounds max_depth eta gamma
2     350         4 0.1     0
  colsample_bytree min_child_weight
2                1                1
  subsample
2         1

#evaluate
ggplot(xgb_N_All2 )
#nrounds = 350, eta = 0.1, and max_depth=4 had lowest RMSE
#


# Fixing colsample bytree, min_child_weight and subsampling (or column and row sampling = subspace sampling), 
xgb_trc_N_All = trainControl(
  method = "repeatedcv",
  number = 5,  
  #repeats = 10, ## dihilangkan buat menambah kecepatan
  allowParallel = TRUE,
  verboseIter = FALSE,
  returnData = FALSE
)

nrounds <- 500
xgbGrid_N_All3 <- expand.grid(
  nrounds = seq(from = 350, to = nrounds, by = 25),
  eta = c(0.1),
  max_depth = c(4),
  gamma = 0,
  colsample_bytree = c(0.4, 0.6, 0.8, 1.0),
  min_child_weight = c(1,2,3),
  subsample = c(0.5, 0.75, 1.0)
)

set.seed(42) 
xgb_N_All3 = train(
	x 		= x_train_N_all,
	y 		= y_train_N_all, 
  trControl = xgb_trc_N_All,
  tuneGrid = xgbGrid_N_All3,
  method = "xgbTree",
  metric = 'RMSE',
  verbosity = 0
)

xgb_N_All3$bestTune
			nrounds max_depth eta gamma
		154     500         4 0.1     0
			colsample_bytree
		154              0.8
			min_child_weight subsample
		154                2       0.5
		
#evaluate
ggplot(xgb_N_All3 )
# subsample = 0.5, colsample_bytree = 0.8, and min_child_weight = 2 yielded lowest RMSE
# nrounds is required to be increase (>500)
# 



# adding higher nrounds, evaluate gamma
xgb_trc_N_All = trainControl(
  method = "repeatedcv",
  number = 5,  
  repeats = 10, 
  allowParallel = TRUE,
  verboseIter = FALSE,
  returnData = FALSE
)

nrounds <- 1000
xgbGrid_N_All4 <- expand.grid(
  nrounds = seq(from = 400, to = nrounds, by = 100),
  eta = c(0.1),
  max_depth = c(4),
  gamma = c(0,1),
  colsample_bytree = c(0.8),
  min_child_weight = c(2),
  subsample = c( 0.5)
)


set.seed(42) 
xgb_N_All4 = train(
	x 		= x_train_N_all,
	y 		= y_train_N_all, 
  trControl = xgb_trc_N_All,
  tuneGrid = xgbGrid_N_All4,
  method = "xgbTree",
  metric = 'RMSE',
  verbosity = 0
)

xgb_N_All4$bestTune
  nrounds max_depth eta gamma
7    1000         4 0.1     0
  colsample_bytree min_child_weight
7              0.8                2
  subsample
7       0.5

#evaluate
ggplot(xgb_N_All4 )
# lowest RMSE is attained at gamma=0
# nrounds need to be increase

# final XGB model
xgb_trc_N_All = trainControl(
  method = "repeatedcv",
  number = 5,  
  repeats = 10, 
  allowParallel = TRUE,
  verboseIter = FALSE,
  returnData = FALSE
)

nrounds <- 10000
xgbGrid_N_All5 <- expand.grid(
  nrounds = seq(from = 1000, to = nrounds, by = 1000),
  eta = c(0.1),
  max_depth = c(4),
  gamma = c(0),
  colsample_bytree = c(0.8),
  min_child_weight = c(2),
  subsample = c( 0.5)
)


set.seed(42) 
xgb_N_All5 = train(
	x 		= x_train_N_all,
	y 		= y_train_N_all, 
  trControl = xgb_trc_N_All,
  tuneGrid = xgbGrid_N_All5,
  method = "xgbTree",
  metric = 'RMSE',
  verbosity = 0
)

xgb_N_All5$bestTune
   nrounds max_depth eta gamma
10   10000         4 0.1     0
   colsample_bytree min_child_weight
10              0.8                2
   subsample
10       0.5

#evaluate (again -_-)

ggplot(xgb_N_All5 )


preds_XGB_N_All_Final <- predict(xgb_N_All, as.matrix(x_test_N_all))

modelEval_XGB_N_All <- cbind(y_test_N_all, preds_XGB_N_All_Final)
colnames(modelEval_XGB_N_All) <- c('Actual', 'Predicted')
modelEval_XGB_N_All <- as.data.frame(modelEval_XGB_N_All)

mse_XGB_N_All <- mean((modelEval_XGB_N_All$Actual - modelEval_XGB_N_All$Predicted)^2)
rmse_XGB_N_All <- sqrt(mse_XGB_N_All)
rmse_XGB_N_All
0.001826508

mae_XGB_N_All <- ModelMetrics::mae(y_test_N_all, preds_XGB_N_All_Final)
mae_XGB_N_All
0.0008722181

bias_XGB_N_All <- Metrics::bias(y_test_N_all, preds_XGB_N_All_Final)
bias_XGB_N_All
0.000154254

#visualise model agreement
# 1 by calobration (internally computed k-folds CV)

models_compare_N <- resamples(list(LM=MLR_N_All, LogGLM=tuned_log_N_All, MARS=tuned_mars_N_All, 
									CUBIST=tuned_cubist_N_All1, RF=RF_N_caret2, GBM=gbm_N_All_caret5, XGB=xgb_N_All5 ))

# Summary of the models performances

summary(models_compare_N)
		Call:
		summary.resamples(object = models_compare_N)

		Models: LM, LogGLM, MARS, CUBIST, RF, GBM, XGB 
		Number of resamples: 50 

		MAE 
					   Min.      1st Qu.      Median        Mean     3rd Qu.       Max. NAs
		LM     1.032865e-01 0.1148018947 0.120326528 0.120344572 0.125875765 0.13623399    0
		LogGLM 9.862711e-02 0.1149822719 0.119555414 0.120024818 0.125849875 0.13595484    0
		MARS   4.889241e-02 0.0674701457 0.075285976 0.074535828 0.081561795 0.09728019    0
		CUBIST 1.294117e-08 0.0008662386 0.002899379 0.005239690 0.008153754 0.03007550    0
		RF     1.229066e-02 0.0167679942 0.018790539 0.019591570 0.021505100 0.03763588    0
		GBM    4.936983e-17 0.0007629146 0.002728294 0.004106446 0.005963334 0.01654051    0
		XGB    3.320272e-04 0.0012759127 0.002472493 0.004061510 0.005594597 0.02185559    0

		RMSE 
					   Min.     1st Qu.     Median       Mean    3rd Qu.       Max. NAs
		LM     1.287069e-01 0.140892475 0.14758406 0.14757521 0.15482956 0.16977672    0
		LogGLM 1.220109e-01 0.142681625 0.14533716 0.14708374 0.15385505 0.16706756    0
		MARS   6.865035e-02 0.094535868 0.10046067 0.10461007 0.11506853 0.13963376    0
		CUBIST 3.352209e-08 0.005935388 0.01722910 0.02691347 0.03662521 0.12347910    0
		RF     1.809156e-02 0.029860963 0.03491292 0.03695258 0.04241209 0.08535481    0
		GBM    7.549443e-17 0.007072756 0.01715688 0.01947360 0.02964906 0.06147276    0
		XGB    4.953687e-04 0.006824174 0.01365972 0.01908128 0.02578449 0.09405303    0

		Rsquared 
					Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NAs
		LM     0.1496865 0.2096485 0.2616713 0.2687833 0.3198168 0.4689396    0
		LogGLM 0.1388341 0.2225834 0.2692825 0.2702196 0.3049343 0.4340512    0
		MARS   0.4255194 0.5683839 0.6454163 0.6290887 0.6893031 0.8398800    0
		CUBIST 0.5563984 0.9501780 0.9910410 0.9533285 0.9987970 1.0000000    0
		RF     0.7576628 0.9472108 0.9675881 0.9544924 0.9756922 0.9908121    0
		GBM    0.8751239 0.9716733 0.9900660 0.9783098 0.9983538 1.0000000    0
		XGB    0.7413845 0.9790914 0.9936214 0.9770633 0.9984596 0.9999919    0


scales <- list(x=list(relation="free"), y=list(relation="free"))
bwplot(models_compare_N, scales=scales)

ggplot(models_compare_N, scales=scales)

models_compare_N$values %>% #extract the values
  select(1, ends_with("RMSE")) %>% #select the first column and all columns with a name ending with "RMSE"
  gather(model, RMSE, -1) %>% #convert to long table
  mutate(model = sub("~RMSE", "", model)) %>% #leave just the model names
  ggplot()+ #call ggplot
  geom_boxplot(aes(x = RMSE, y = model, fill=model)) -> plotN1 #and plot the box plot

plotN1_fix <- plotN1 + scale_y_discrete(limits = c("LM", "LogGLM", "MARS", "CUBIST", "RF", "GBM", "XGB"))+
    scale_colour_brewer(palette = "Oranges") + theme_bw()  + 
	theme(text=element_text(size=13, family="Palatino"),
	axis.title.y=element_blank())+theme( legend.position="none")

models_compare_N$values %>% #extract the values
  select(1, ends_with("MAE")) %>% #select the first column and all columns with a name ending with "MAE"
  gather(model, MAE, -1) %>% #convert to long table
  mutate(model = sub("~MAE", "", model)) %>% #leave just the model names
  ggplot()+ #call ggplot
  geom_boxplot(aes(x = MAE, y = model, fill=model)) -> plotN2 #and plot the box plot

plotN2_fix <- plotN2 + scale_y_discrete(limits = c("LM", "LogGLM", "MARS", "CUBIST", "RF", "GBM", "XGB"))+
    scale_colour_brewer(palette = "Oranges")  + theme_bw()  + 
	theme(text=element_text(size=13, family="Palatino"),
	axis.title.y=element_blank(),
	axis.text.y=element_blank())+theme( legend.position="none")

models_compare_N$values %>% #extract the values
  select(1, ends_with("Rsquared")) %>% #select the first column and all columns with a name ending with "Rsquared"
  gather(model, Rsquared, -1) %>% #convert to long table
  mutate(model = sub("~Rsquared", "", model)) %>% #leave just the model names
  ggplot()+ #call ggplot
  geom_boxplot(aes(x = Rsquared, y = model, fill=model)) -> plotN3 #and plot the box plot

plotN3_fix <- plotN3 + scale_y_discrete(limits = c("LM", "LogGLM", "MARS", "CUBIST", "RF", "GBM", "XGB"))+
    scale_colour_brewer(palette = "Oranges")  + theme_bw() + 
	theme(text=element_text(size=13, family="Palatino"),
	axis.title.y=element_blank(),
	axis.text.y=element_blank())+theme( legend.position="none")


plotN1_fix+plotN2_fix+plotN3_fix


#performance by validation method

# 2 by validation (externally by the 30% validation data)

									
preds_MLR_N_All1 <- predict(MLR_N_All, test_N_all)
preds_LOG_GLM_N_All1 <- predict(tuned_log_N_All, test_N_all)
preds_MARS_N_All1 <- predict(tuned_mars_N_All, test_N_all)
preds_CUBIST_N_All1 <- predict(tuned_cubist_N_All1, test_N_all)
preds_RF_N_All_Final1 <- predict(RF_N_caret2, test_N_all)
preds_GBM_N_All1 <- predict(gbm_N_All_caret1, test_N_all)
preds_XGB_N_All_Final1 <- predict(xgb_N_All, as.matrix(x_test_N_all))


MLR_df_p <- data.frame(	Rsq = R2(preds_MLR_N_All1, y_test_N_all),  
						RMSE = RMSE(preds_MLR_N_All1, y_test_N_all),  
						MAE = MAE(preds_MLR_N_All1, y_test_N_all),
						BIAS = Metrics::bias(y_test_N_all, preds_MLR_N_All1)) %>% add_column(Model="MLR")


LogGLM_df_p <- data.frame(	Rsq = R2(preds_LOG_GLM_N_All1, y_test_N_all),  
						RMSE = RMSE(preds_LOG_GLM_N_All1, y_test_N_all),  
						MAE = MAE(preds_LOG_GLM_N_All1, y_test_N_all),
						BIAS = Metrics::bias(y_test_N_all, preds_LOG_GLM_N_All1))%>% add_column(Model="LogGLM")

MARS_df_p <- data.frame(Rsq = R2(preds_MARS_N_All1, y_test_N_all),  
						RMSE = RMSE(preds_MARS_N_All1, y_test_N_all),  
						MAE = MAE(preds_MARS_N_All1, y_test_N_all),
						BIAS = Metrics::bias(y_test_N_all, preds_MARS_N_All1))%>% add_column(Model="MARS")

names(MARS_df_p)[names(MARS_df_p) == 'y'] <- 'Rsq'

CUBIST_df_p <- data.frame(	Rsq = R2(preds_CUBIST_N_All1, y_test_N_all),  
						RMSE = RMSE(preds_CUBIST_N_All1, y_test_N_all),  
						MAE = MAE(preds_CUBIST_N_All1, y_test_N_all),
						BIAS = Metrics::bias(y_test_N_all, preds_CUBIST_N_All1))%>% add_column(Model="CUBIST")

RF_df_p <- data.frame(	Rsq = R2(preds_RF_N_All_Final1, y_test_N_all),  
						RMSE = RMSE(preds_RF_N_All_Final1, y_test_N_all),  
						MAE = MAE(preds_RF_N_All_Final1, y_test_N_all),
						BIAS = Metrics::bias(y_test_N_all, preds_RF_N_All_Final1))%>% add_column(Model="RF")
						
GBM_df_p <- data.frame(	Rsq = R2(preds_GBM_N_All1, y_test_N_all),  
						RMSE = RMSE(preds_GBM_N_All1, y_test_N_all),  
						MAE = MAE(preds_GBM_N_All1, y_test_N_all),
						BIAS = Metrics::bias(y_test_N_all, preds_GBM_N_All1))	%>% add_column(Model="GBM")					
						
XGB_df_p <- data.frame(	Rsq = R2(preds_XGB_N_All_Final1, y_test_N_all),  
						RMSE = RMSE(preds_XGB_N_All_Final1, y_test_N_all),  
						MAE = MAE(preds_XGB_N_All_Final1, y_test_N_all),
						BIAS = Metrics::bias(y_test_N_all, preds_XGB_N_All_Final1))	%>% add_column(Model="XGB")					
						

merged_all_model <- bind_rows(MLR_df_p, LogGLM_df_p,MARS_df_p,CUBIST_df_p, RF_df_p,GBM_df_p, XGB_df_p)
merged_all_model 
				Rsq         RMSE          MAE          BIAS  Model
		1 0.3289702 1.401671e-01 1.148419e-01  9.123912e-05    MLR
		2 0.3365425 1.393914e-01 1.141461e-01 -1.055489e-04 LogGLM
		3 0.7199123 9.123466e-02 6.408076e-02 -4.937763e-03   MARS
		4 1.0000000 1.213287e-07 2.328173e-08  6.707359e-09 CUBIST
		5 0.9917447 1.681139e-02 7.871883e-03  1.398181e-03     RF
		6 0.9999992 1.591850e-04 1.017097e-04  5.978220e-06    GBM
		7 0.9999009 1.826508e-03 8.722181e-04  1.542540e-04    XGB


## Explaining ML with fastshap
# set Monte Carlo Simulation to 1000

shap_mlr <- fastshap::explain(
  MLR_N_All, 
  X = X_df_train,
  nsim = 1000, 
  pred_wrapper = predict,
  shap_only = FALSE,
  adjust = TRUE
)

	
shap_LogGLM <- fastshap::explain(
  tuned_log_N_All, 
  X = X_df_train,
  nsim = 1000, 
  pred_wrapper = predict,
  shap_only = FALSE,
  adjust = TRUE
)


#fail
X_df_train_mars <- data.frame(subset(X_df_train, select = Depth) %>% as.matrix() )
shap_MARS <- fastshap::explain(
  tuned_mars_N_All, 
  X = X_df_train_mars,
  nsim = 1000, 
  pred_wrapper = predict,
  shap_only = FALSE,
  adjust = TRUE
)
sv_MARS <- shapviz(shap_MARS)
	

shap_Cubist <- fastshap::explain(
  tuned_cubist_N_All1, 
  X = X_df_train,
  nsim = 1000, 
  pred_wrapper = predict,
  shap_only = FALSE,
  adjust = TRUE
)


shap_RF <- fastshap::explain(
  RF_N_All_Final, 
  X = X_df_train,
  nsim = 1000, 
  pred_wrapper = predict,
  shap_only = FALSE,
  adjust = TRUE
)


shap_GBM <- fastshap::explain(
  gbm_N_All_caret1, 
  X = X_df_train,
  nsim = 1000, 
  pred_wrapper = predict,
  shap_only = FALSE,
  adjust = TRUE
)


shap_XGB <- fastshap::explain(
  xgb_N_All, 
  X = X_df_train,
  nsim = 1000, 
  pred_wrapper = predict,
  shap_only = FALSE,
  adjust = TRUE
)


## transfer fastshap to shapviz object

sv_LM <- shapviz(shap_mlr)
sv_LogGLM <- shapviz(shap_LogGLM)
sv_Cubist <- shapviz(shap_Cubist)
sv_RF <- shapviz(shap_RF)
sv_GBM <- shapviz(shap_GBM)
sv_XGB <- shapviz(shap_XGB)


## Waterfall Plot

sv_LM_wtf <- sv_waterfall(sv_LM, row_id = 3, fill_colors=c('#FF0051', '#008BFB')) +ggtitle("LM, row_id=3")+
							theme(text=element_text(size=14,family="Palatino"),
							axis.title.x=element_blank())
sv_LM_wtf$mapping$linetype <- NA

sv_LogGLM_wtf <- sv_waterfall(sv_LogGLM, row_id = 3, fill_colors=c('#FF0051', '#008BFB')) +ggtitle("LogGLM, row_id=3")+
							theme(text=element_text(size=14,family="Palatino"),
							axis.title.x=element_blank())							
sv_LogGLM_wtf$mapping$linetype <- NA

sv_Cubist_wtf <- sv_waterfall(sv_Cubist, row_id = 3, fill_colors=c('#FF0051', '#008BFB')) +ggtitle("CUBIST row_id=3")+
							theme(text=element_text(size=14,family="Palatino"),
							axis.title.x=element_blank())
sv_Cubist_wtf$mapping$linetype <- NA

sv_RF_wtf <- sv_waterfall(sv_RF, row_id = 3, fill_colors=c('#FF0051', '#008BFB')) +ggtitle("RF, row_id=3")+
							theme(text=element_text(size=14,family="Palatino"),
							axis.title.x=element_blank())	
sv_RF_wtf$mapping$linetype <- NA

sv_GBM_wtf <- sv_waterfall(sv_GBM, row_id = 3, fill_colors=c('#FF0051', '#008BFB')) +ggtitle("GBM, row_id=3")+
							theme(text=element_text(size=14,family="Palatino"),
							axis.title.x=element_blank())	
sv_GBM_wtf$mapping$linetype <- NA

sv_XGB_wtf <- sv_waterfall(sv_XGB, row_id = 3, fill_colors=c('#FF0051', '#008BFB')) +ggtitle("XGB, row_id=3")+
							theme(text=element_text(size=14,family="Palatino"))	
sv_XGB_wtf$mapping$linetype <- NA


sv_LM_wtf+sv_LogGLM_wtf+sv_RF_wtf+sv_GBM_wtf+sv_XGB_wtf+plot_layout(nrow=3)


sv_LogGLM_wtf2 <- sv_waterfall(sv_LogGLM, row_id = 100, fill_colors=c('#FF0051', '#008BFB')) +ggtitle("LogGLM, row_id=100")+
							theme(text=element_text(size=14,family="Palatino"))
														
sv_LogGLM_wtf2$mapping$linetype <- NA

sv_GBM_wtf2 <- sv_waterfall(sv_GBM, row_id = 100, fill_colors=c('#FF0051', '#008BFB')) +ggtitle("GBM, row_id=100")+
							theme(text=element_text(size=14,family="Palatino"))	
sv_GBM_wtf2$mapping$linetype <- NA

sv_LogGLM_wtf+sv_GBM_wtf+
sv_LogGLM_wtf2+sv_GBM_wtf2+
plot_layout(ncol=2)

## Beeswarm Plot

bee_LM <- sv_importance(sv_LM, kind = "beeswarm", show_numbers = FALSE, color_bar_title=NULL)+theme_bw()+ 
							 ggtitle("LM")+
							theme(axis.text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.title.x=element_blank(),
							axis.text.y=element_blank())
							
bee_LogGLM <- sv_importance(sv_LogGLM, kind = "beeswarm", show_numbers = FALSE)+
							theme_bw()+ 
							 ggtitle("LogGLM")+
							theme(axis.text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.title.x=element_blank(),
							axis.text.y=element_blank())

bee_Cubist <- sv_importance(sv_Cubist, kind = "beeswarm", show_numbers = FALSE,  color_bar_title=NULL)+
							theme_bw()+ 
							 ggtitle("Cubist") +
							theme(axis.text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.title.x=element_blank(),
							axis.text.y=element_blank()	)

bee_RF <- sv_importance(sv_RF, kind = "beeswarm", show_numbers = FALSE,  color_bar_title=NULL)+
							theme_bw()+ 
							 ggtitle("RF") +
							theme(axis.text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.title.x=element_blank(),
							axis.text.y=element_blank()	)
							
bee_GBM <- sv_importance(sv_GBM, kind = "beeswarm", show_numbers = FALSE)+
							theme_bw()+ 
							 ggtitle("GBM")+
							theme(axis.text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.text.y=element_blank())
							
bee_XGB <- sv_importance(sv_XGB, kind = "beeswarm", show_numbers = FALSE)+
							theme_bw()+ 
							 ggtitle("XGB")+
							theme(axis.text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.text.y=element_blank())

bee_LM+bee_LogGLM+bee_Cubist+bee_RF+bee_GBM+bee_XGB+plot_layout(nrow=3)


bee_LogGLM+bee_GBM

# Importance (mean) Plot

svMean_LM <- sv_importance(sv_LM, kind = "bar", show_numbers = FALSE, color_bar_title=NULL, fill="#674ea7")+theme_bw()+ 
							 ggtitle("LM")+ 
							theme(axis.text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.title.x=element_blank())
							
svMean_LogGLM <- sv_importance(sv_LogGLM, kind = "bar", show_numbers = FALSE,  color_bar_title=NULL, fill="#674ea7")+
							theme_bw()+ 
							 ggtitle("LogGLM")+
							theme(axis.text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.title.x=element_blank(),
							axis.title.y=element_blank())							

svMean_Cubist <- sv_importance(sv_Cubist, kind = "bar", show_numbers = FALSE,  color_bar_title=NULL, fill="#674ea7")+
							theme_bw()+ 
							 ggtitle("Cubist") +
							theme(axis.text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.title.x=element_blank())

svMean_RF <- sv_importance(sv_RF, kind = "bar", show_numbers = FALSE,  color_bar_title=NULL, fill="#674ea7")+
							theme_bw()+ 
							 ggtitle("RF") +
							theme(axis.text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.title.x=element_blank())
							
svMean_GBM <- sv_importance(sv_GBM, kind = "bar", show_numbers = FALSE, , fill="#674ea7")+
							theme_bw()+ 
							 ggtitle("GBM")+
							theme(axis.text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.title.y=element_blank())
							
svMean_XGB <- sv_importance(sv_XGB, kind = "bar", show_numbers = FALSE, , fill="#674ea7")+
							theme_bw()+ 
							 ggtitle("XGB")+
							theme(axis.text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.title.y=element_blank())

svMean_LM+svMean_LogGLM+svMean_Cubist+svMean_RF+svMean_GBM+svMean_XGB+plot_layout(nrow=3)


svMean_LogGLM+bee_LogGLM+svMean_GBM+bee_GBM+plot_layout(nrow=2)




## dependence plot


dep_mlr_depth <- sv_dependence(sv_LM, v = "Depth")+
						theme_bw()+ 
							 ggtitle("LM")+
							 labs(x = " Depth (cm)")+
							theme(text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"))+
							scale_color_continuous(guide = guide_colorbar(
							ticks = FALSE, barheight = 10), type = "viridis")
							
dep_LogGLM_depth <- sv_dependence(sv_LogGLM, v = "Depth")+
						theme_bw()+
							 ggtitle("LogGLM")+
							 labs(x = " Depth (cm)")+
							theme(text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.title.y=element_text(size = 13 ,family="Palatino"))+
							scale_color_continuous(guide = guide_colorbar(
							ticks = FALSE, barheight = 10), type = "viridis") 
							
dep_RF_depth <- sv_dependence(sv_RF, v = "Depth")+
						theme_bw()+
							 ggtitle("RF")+
							 labs(x = " Depth (cm)")+
							theme(text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.title.y=element_blank())+
							scale_color_continuous(guide = guide_colorbar(
							ticks = FALSE, barheight = 10), type = "viridis")

dep_GBM_depth <- sv_dependence(sv_GBM, v = "Depth")+
						theme_bw()+
							 ggtitle("GBM")+
							 labs(x = " Depth (cm)")+
							theme(text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.title.y=element_blank())+
							scale_color_continuous(guide = guide_colorbar(
							ticks = FALSE, barheight = 10), type = "viridis")

dep_XGB_depth <- sv_dependence(sv_XGB, v = "Depth")+
						theme_bw()+
							 ggtitle("XGB")+
							 labs(x = " Depth (cm)")+
							theme(text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.title.y=element_blank())+
							scale_color_continuous(guide = guide_colorbar(
							ticks = FALSE, barheight = 10), type = "viridis")



dep_mlr_depth+dep_LogGLM_depth+dep_RF_depth+dep_GBM_depth+dep_XGB_depth
dep_LogGLM_depth+dep_GBM_depth

dep_mlr_OC <- sv_dependence(sv_LM, v = "Org_C")+
						theme_bw()+ 
							 ggtitle("LM")+
							 labs(x = " Organic C (%)")+
							theme(text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"))+
							scale_color_continuous(guide = guide_colorbar(
							ticks = FALSE, barheight = 10), type = "viridis")				

						
dep_LogGLM_OC <- sv_dependence(sv_LogGLM, v = "Org_C")+
						theme_bw()+
				#			 ggtitle("")+
							 labs(x = " Organic C (%)")+
							theme(text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.title.y=element_text(size = 13 ,family="Palatino"))+
							scale_color_continuous(guide = guide_colorbar(
							ticks = FALSE, barheight = 10), type = "viridis")
							
dep_RF_OC <- sv_dependence(sv_RF, v = "Org_C")+
						theme_bw()+
							 ggtitle("RF")+
							 labs(x = " Organic C (%)")+
							theme(text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.title.y=element_blank())+
							scale_color_continuous(guide = guide_colorbar(
							ticks = FALSE, barheight = 10), type = "viridis")
							
dep_GBM_OC <- sv_dependence(sv_GBM, v = "Org_C")+
						theme_bw()+
				#			 ggtitle("")+
							 labs(x = " Organic C (%)")+
							theme(text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.title.y=element_blank())+
							scale_color_continuous(guide = guide_colorbar(
							ticks = FALSE, barheight = 10), type = "viridis")

dep_XGB_OC <- sv_dependence(sv_XGB, v = "Org_C")+
						theme_bw()+
							 ggtitle("XGB")+
							 labs(x = " Organic C (%)")+
							theme(text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.title.y=element_blank())+
							scale_color_continuous(guide = guide_colorbar(
							ticks = FALSE, barheight = 10), type = "viridis")



dep_mlr_OC+dep_LogGLM_OC+dep_RF_OC+dep_GBM_OC+dep_XGB_OC

dep_LogGLM_depth+dep_GBM_depth+
dep_LogGLM_OC+dep_GBM_OC


dep_mlr_Total_Fe <- sv_dependence(sv_LM, v = "Total_Fe")+
						theme_bw()+ 
							 ggtitle("LM")+
							 labs(x = " Total Fe (mg/kg)")+
							theme(text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"))+
							scale_color_continuous(guide = guide_colorbar(
							ticks = FALSE, barheight = 10), type = "viridis")
							
dep_LogGLM_Total_Fe <- sv_dependence(sv_LogGLM, v = "Total_Fe")+
						theme_bw()+
						#	 ggtitle("")+
							 labs(x = " Total Fe (mg/kg)")+
							theme(text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.title.y=element_text(size = 13 ,family="Palatino"))+
							scale_color_continuous(guide = guide_colorbar(
							ticks = FALSE, barheight = 10), type = "viridis") 
							
dep_RF_Total_Fe <- sv_dependence(sv_RF, v = "Total_Fe")+
						theme_bw()+
							 ggtitle("RF")+
							 labs(x = " Total Fe (mg/kg)")+
							theme(text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.title.y=element_blank())+
							scale_color_continuous(guide = guide_colorbar(
							ticks = FALSE, barheight = 10), type = "viridis")

dep_GBM_Total_Fe <- sv_dependence(sv_GBM, v = "Total_Fe")+
						theme_bw()+
					#		 ggtitle("")+
							 labs(x = " Total Fe (mg/kg)")+
							theme(text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.title.y=element_blank())+
							scale_color_continuous(guide = guide_colorbar(
							ticks = FALSE, barheight = 10), type = "viridis")

dep_XGB_Total_Fe <- sv_dependence(sv_XGB, v = "Total_Fe")+
						theme_bw()+
							 ggtitle("XGB")+
							 labs(x = " Total Fe (mg/kg)")+
							theme(text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.title.y=element_blank())+
							scale_color_continuous(guide = guide_colorbar(
							ticks = FALSE, barheight = 10), type = "viridis")


dep_mlr_Total_Fe+dep_LogGLM_Total_Fe+dep_RF_Total_Fe+dep_GBM_Total_Fe+dep_XGB_Total_Fe

dep_LogGLM_depth+dep_GBM_depth+
dep_LogGLM_OC+dep_GBM_OC +
dep_LogGLM_Total_Fe+dep_GBM_Total_Fe+plot_layout(nrow=3)



dep_mlr_BD <- sv_dependence(sv_LM, v = "BD")+
						theme_bw()+ 
							 ggtitle("LM")+
							 labs(x = " BD (g/cm3)")+
							theme(text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"))+
							scale_color_continuous(guide = guide_colorbar(
							ticks = FALSE, barheight = 10), type = "viridis")
							
dep_LogGLM_BD <- sv_dependence(sv_LogGLM, v = "BD")+
						theme_bw()+
				#			 ggtitle("LogGLM")+
							 labs(x = " BD (g/cm3)")+
							theme(text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.title.y=element_text(size = 13 ,family="Palatino"))+
							scale_color_continuous(guide = guide_colorbar(
							ticks = FALSE, barheight = 10), type = "viridis") 
							
dep_RF_BD <- sv_dependence(sv_RF, v = "BD")+
						theme_bw()+
							 ggtitle("RF")+
							 labs(x = " BD (g/cm3)")+
							theme(text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.title.y=element_blank())+
							scale_color_continuous(guide = guide_colorbar(
							ticks = FALSE, barheight = 10), type = "viridis")

dep_GBM_BD <- sv_dependence(sv_GBM, v = "BD")+
						theme_bw()+
			#				 ggtitle("GBM")+
							 labs(x = " BD (g/cm3)")+
							theme(text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.title.y=element_blank())+
							scale_color_continuous(guide = guide_colorbar(
							ticks = FALSE, barheight = 10), type = "viridis")

dep_XGB_BD <- sv_dependence(sv_XGB, v = "BD")+
						theme_bw()+
							 ggtitle("XGB")+
							 labs(x = " BD (g/cm3)")+
							theme(text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.title.y=element_blank())+
							scale_color_continuous(guide = guide_colorbar(
							ticks = FALSE, barheight = 10), type = "viridis")

dep_mlr_BD+dep_LogGLM_BD+dep_RF_BD+dep_GBM_BD+dep_XGB_BD

dep_LogGLM_depth+dep_GBM_depth+
dep_LogGLM_OC+dep_GBM_OC +
dep_LogGLM_Total_Fe+dep_GBM_Total_Fe+
dep_LogGLM_BD+dep_GBM_BD +plot_layout(nrow=4)
























