library(tidyverse) # plotting and manipulation
library(grid) # combining plots
library(gridExtra) # combining plots
library(ggpubr) # combining plots
library(patchwork) # combining plots
library(ggfortify) # nice extension for ggplot
library(mgcv) #fitting gam models
library(GGally) # displaying pairs panel
library(caTools) # split dataset
library(readxl)
library(randomForest)
library(e1071)
library(gbm)          # basic implementation
library(xgboost)      # a faster implementation of gbm
library(caret)        # an aggregator package for performing many machine learning models
library(pdp)          # model visualization
library(lime)         # model visualization
library(neuralnet)
library(rpart)     #rpart for computing decision tree models
library(rsample)     # data splitting 
library(dplyr)       # data wrangling
library(rpart.plot)  # plotting regression trees
library(ipred)       # bagging
library(broom)
library(ranger) 	#efficient RF
library(NeuralNetTools)
library(tidymodels)
library(earth) 		#MARS model
library(iml)		#most robust and efficient relative importance 
library(xgboost)	#extreeme gradient boosting
library(ModelMetrics) #get model metrics
library(Metrics) 	#get ML model metrics
library(Cubist) #Cubist model
library(FactoMineR)#PCA
library(factoextra) #PCA
library(rattle)
library(vip)
library(shapper)
library("DALEX2")
library(viridis)
library(tidytext)
library(fastshap)
library(shapviz)
library(readxl)
library(boot)

windowsFonts(Palatino=windowsFont("Palatino Linotype")) #setup font type to comply the Journal's font

library(readxl)
N_all_fix <- read_excel("D:/Publikasi/Pak Heru B Pulunggono/16 Machine Learning for N in peat yusuf/data/R_fix_yusuf.xlsx")
View(N_all_fix)
str(N_all_fix)


N_all_fix$Org_C <- N_all_fix$Org_C *100		#convert Org-C to percent value 
N_all_fix <- na.omit(N_all_fix)			#omit missing values

#other preprocessing had been done outside of R (resulting in N.A)
str(N_all_fix)


#histogram frequency for original dataset
plot_dist_N1 <- N_all_fix %>% 
				ggplot( aes(x=Total_N)) +
				geom_density(fill="#69b3a2", color="#e9ecef", alpha=0.8)+
				#geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity') +
				theme_bw()+
				labs(x = " Total N (%)", y = "Frequency")+
				theme(text=element_text(size=13,family="Palatino"))
								
plot_dist_N1


## bootstrapping with replacement 
boots_N <- N_all_fix %>% 
  bind_rows(
    N_all_fix %>% 
      sample_n(500,replace = TRUE) 
  )
  
str(boots_N)

# Correlation using ggally package

N_all_fix_corr = subset(N_all_fix, select = -c(1:6)) #subset to eliminate one-hot-encoded factors
str(N_all_fix_corr)

#build a custom function for more flexibility to adjust ggally

my_fn_corr <- function(data, mapping, method="p", use="pairwise", ...){

              # grab data
              x <- eval_data_col(data, mapping$x)
              y <- eval_data_col(data, mapping$y)

              # calculate correlation
              corr <- cor(x, y, method=method, use='complete.obs')

              # calculate colour based on correlation value
              # Here I have set a correlation of minus one to blue, 
              # zero to white, and one to red 
              # Change this to suit: possibly extend to add as an argument of `my_fn_corr`
              colFn <- colorRampPalette(c("dodgerblue", "darkseagreen1", "green4"), interpolate ='spline')
              fill <- colFn(100)[findInterval(corr, seq(-1, 1, length=100))]

              ggally_cor(data = data, mapping = mapping, ...) + 
                theme_void() +
                theme(panel.background = element_rect(fill=fill))
            }

#correlation using original dataset --> to appendix
p1_corr <- ggpairs(N_all_fix_corr, 
                   upper = list(continuous = my_fn_corr),
                   lower = list(continuous = "smooth"))  			
p1_corr+theme(text = element_text(size=11, family="Palatino"))			


#correlation using bootstrapped sampling --> to article body
N_all_boot_corr = subset(boots_N, select = -c(1:6))

p1_boot_corr <- ggpairs(N_all_boot_corr, 
                   upper = list(continuous = my_fn_corr),
                   lower = list(continuous = "smooth"))  			
p1_boot_corr+theme(text = element_text(size=11, family="Palatino"))	


## Modelling

#sample split (70:30)
set.seed(42)

Split_N_all <- sample.split(Y=boots_N$Total_N, SplitRatio=0.7)
train_N_all<- subset(x=boots_N, Split_N_all==TRUE)
test_N_all <- subset(x=boots_N, Split_N_all==FALSE)

x_train_N_all = subset(train_N_all, select = -Total_N) %>% as.matrix() 
y_train_N_all = train_N_all$Total_N

x_test_N_all = subset(test_N_all, select = -Total_N) %>% as.matrix() 
y_test_N_all = test_N_all$Total_N 

X_df_train <- data.frame(x_train_N_all)
x_df_test <- data.frame(x_test_N_all)
train_N_all_df <- data.frame(train_N_all)
test_N_all_df <-data.frame(test_N_all)

summary(train_N_all)

summary(test_N_all )

##visualize distribution 
N_train <- train_N_all%>%
  add_column(Status = "Training")

N_test <- test_N_all%>%
  add_column(Status = "Validation")

N_df <- bind_rows(N_train, N_test)

str(N_df)

windowsFonts(Palatino=windowsFont("Palatino Linotype"))

# Represent it
plot_dist_N <- N_df %>% 
				ggplot( aes(x=Total_N, color=Status, fill=Status)) +
				geom_density(alpha = 0.5, linewidth=0.8)+
				#geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity') +
				scale_fill_manual(values=c("#69b3a2", "#404080")) +
				scale_color_manual(values=c("#69b3a2", "#404080")) +
				theme_bw()+
				labs(x = " Total N (%)", y = "Frequency")+
				theme(text=element_text(size=13,family="Palatino"))
				
				
plot_dist_N


baru_N <-  N_df%>% subset(,-c(1:7))%>%
		pivot_longer(cols = -"Status", names_to="Property", values_to="value")

str(baru_N)	


IndvarNames1 = list(
'BD'="BD (g/cm3)",
'Org_C'="Org-C (%)",
'PD'="PD (g/cm3)",
'pH'="pH",
'Total_N'="Total N (%)",
'Total_Fe' = "Total Fe (mg/kg")

# make label function, passing to facet_wrap ggplot2

indvar_labeller1 <- function(variable,value){
  return(IndvarNames1[value])
}

# outlier elimination function, passing to stat_summary ggplot2

calc_stat1 <- function(x) {
  coef <- 1.5
  n <- sum(!is.na(x))
  # calculate quantiles
  stats <- quantile(x, probs = c(0.1, 0.25, 0.5, 0.75, 0.9))
  names(stats) <- c("ymin", "lower", "middle", "upper", "ymax")
  return(stats)
}

boxplot_N <- baru_N %>%
				ggplot(aes(x=Property, y=value, fill=Status)) +
				stat_summary(aes(fill=factor(Status)), fun.data = calc_stat1, geom="boxplot" ,
								position=position_dodge(width=1.1),alpha = 0.7 ) + 
				facet_wrap(~Property, labeller=indvar_labeller1, scales="free")+
				scale_fill_manual(values=c("#69b3a2", "#404080")) +
				theme_bw()+
				theme(text=element_text(size=13, family="Palatino"),
				axis.text.x=element_blank(),
				axis.title.x=element_blank(),
				axis.title.y=element_blank())

boxplot_N

plot_dist_N+boxplot_N+plot_layout(ncol=1) #merge total N distribution to covariate's boxplots


## modelling on the go

set.seed = 42
MLR_N_All <- train(
  x 			= x_train_N_all,
  y 			= y_train_N_all,
  method = "lm",
  family = "gaussian",
  metric = "RMSE",
  trControl = trainControl(method = "repeatedcv", number = 5, repeats=10)
)

MLR_N_All 



preds_MLR_N_All <- predict(MLR_N_All, test_N_all)

modelEval_MLR_N_All <- cbind(test_N_all$Total_N, preds_MLR_N_All)
colnames(modelEval_MLR_N_All) <- c('Actual', 'Predicted')
modelEval_MLR_N_All <- as.data.frame(modelEval_MLR_N_All)

mse_MLR_N_All <- mean((modelEval_MLR_N_All$Actual - modelEval_MLR_N_All$Predicted)^2)
rmse_MLR_N_All <- sqrt(mse_MLR_N_All)
rmse_MLR_N_All

mae_MLR_N_All <- ModelMetrics::mae(y_test_N_all, preds_MLR_N_All)
mae_MLR_N_All

bias_MLR_N_All <- Metrics::bias(y_test_N_all, preds_MLR_N_All)
bias_MLR_N_All


## Log - GLM regression

tuned_log_N_All <- train(
  x 			= x_train_N_all,
  y 			= y_train_N_all,
  method = "glm",
  family = "binomial",
  metric = "RMSE",
  trControl = trainControl(method = "repeatedcv", number = 5, repeats=10)
)

tuned_log_N_All

preds_LOG_GLM_N_All <- predict(tuned_log_N_All, test_N_all)

modelEval_LOG_GLM_N_All <- cbind(test_N_all$Total_N, preds_LOG_GLM_N_All)
colnames(modelEval_LOG_GLM_N_All) <- c('Actual', 'Predicted')
modelEval_LOG_GLM_N_All <- as.data.frame(modelEval_LOG_GLM_N_All)

mse_LOG_GLM_N_All <- mean((modelEval_LOG_GLM_N_All$Actual - modelEval_LOG_GLM_N_All$Predicted)^2)
rmse_LOG_GLM_N_All <- sqrt(mse_LOG_GLM_N_All)
rmse_LOG_GLM_N_All

mae_log_GLM_N_All <- ModelMetrics::mae(y_test_N_all, preds_LOG_GLM_N_All)
mae_log_GLM_N_All

bias_log_GLM_N_All <- Metrics::bias(y_test_N_all, preds_LOG_GLM_N_All)
bias_log_GLM_N_All

# Multivariate adaptive regression spline/MARS 

#tuning model 1
hyper_grid_mars <- expand.grid(
  degree = 1:5, 
  nprune = seq(2, 100, length.out = 10) %>% floor()
)

set.seed(42)
# cross validated model
tuned_mars_N_All <- train(
  x 			= X_df_train,
  y 			= y_train_N_all,
  method 		= "earth",
  metric 		= "RMSE",
  trControl 	= trainControl(method = "repeatedcv", number = 5, repeats = 10),
  tuneGrid 		= hyper_grid_mars
							)
summary(tuned_mars_N_All)

tuned_mars_N_All$bestTune


hyper_grid_mars2 <- expand.grid(
  degree = 3:5, 
  nprune = seq(20, 30, length.out = 2) %>% floor()
)

#tuning model 2
set.seed(42)
# cross validated model
tuned_mars_N_All2 <- train(
  x 			= X_df_train,
  y 			= y_train_N_all,
  method 		= "earth",
  metric 		= "RMSE",
  trControl 	= trainControl(method = "repeatedcv", number = 5, repeats = 10),
  tuneGrid 		= hyper_grid_mars2
							)
summary(tuned_mars_N_All2)


tuned_mars_N_All2$bestTune


#resamples to get model comparison
res_mars <- resamples(list(mars1 = tuned_mars_N_All, mars2 = tuned_mars_N_All2))
summary(res_mars)		
		
scales_mars <- list(x=list(relation="free"), y=list(relation="free"))

bwplot(res_mars, scales=scales_mars)

ggplot(res_mars, scales=scales_mars)

res_mars$values %>% #extract the values
  select(1, ends_with("Rsquared")) %>% #select the first column and all columns with a name ending with "Rsquared"
  gather(model, Rsquared, -1) %>% #convert to long table
  mutate(model = sub("~Rsquared", "", model)) %>% #leave just the model names
  ggplot()+ #call ggplot
  geom_boxplot(aes(x = Rsquared, y = model, fill=model)) -> plotres_mars #and plot the box plot

plot_res_mars<- plotres_mars + scale_y_discrete(limits = c("mars1", "mars2"))+
    scale_fill_viridis(discrete = TRUE) + theme_bw()  + 
	theme(text=element_text(size=13, family="Palatino"),
	axis.title.y=element_blank())+theme( legend.position="none") 

plot_res_mars #similar performance (both in resamples summary and boxplots)


preds_MARS_N_All <- predict(tuned_mars_N_All, test_N_all)

modelEval_MARS_N_All <- cbind(test_N_all$Total_N, preds_MARS_N_All)
colnames(modelEval_MARS_N_All) <- c('Actual', 'Predicted')
modelEval_MARS_N_All <- as.data.frame(modelEval_MARS_N_All)

mse_MARS_N_All <- mean((modelEval_MARS_N_All$Actual - modelEval_MARS_N_All$Predicted)^2)
rmse_MARS_N_All <- sqrt(mse_MARS_N_All)
rmse_MARS_N_All

mae_mars_N_All <- ModelMetrics::mae(y_test_N_all, preds_MARS_N_All)
mae_mars_N_All

bias_mars_N_All <- Metrics::bias(y_test_N_all, preds_MARS_N_All)
bias_mars_N_All


#Cubist (Quinlan's rule-based)

#tuning1
hyper_grid_cubist1 <- expand.grid(committees = c(1, 10, 50, 100), neighbors = c(0, 1, 5, 9))

set.seed(42)
# cross validated model
tuned_cubist_N_All1 <- train(
    x 			= X_df_train,
  y 			= y_train_N_all,
  method = "cubist",
  tuneGrid = hyper_grid_cubist1,
  trControl = trainControl(method = "repeatedcv", number = 5, repeats = 10)
  )
tuned_cubist_N_All1
	
#tuning2
hyper_grid_cubist2 <- expand.grid(committees = c(50, 100), neighbors = c(1, 2, 3, 4))

set.seed(42)
# cross validated model
tuned_cubist_N_All2 <- train(
    x 			= X_df_train,
  y 			= y_train_N_all,
  method = "cubist",
  tuneGrid = hyper_grid_cubist2,
  trControl = trainControl(method = "repeatedcv", number = 5, repeats = 10)
  )
tuned_cubist_N_All2



res_cubist <- resamples(list(cubist1 = tuned_cubist_N_All1, cubist2 = tuned_cubist_N_All2))
summary(res_cubist)

		
scales_cubist <- list(x=list(relation="free"), y=list(relation="free"))

bwplot(res_cubist, scales=scales_cubist)

ggplot(res_cubist, scales=scales_cubist)

res_cubist$values %>% #extract the values
  select(1, ends_with("Rsquared")) %>% #select the first column and all columns with a name ending with "Rsquared"
  gather(model, Rsquared, -1) %>% #convert to long table
  mutate(model = sub("~Rsquared", "", model)) %>% #leave just the model names
  ggplot()+ #call ggplot
  geom_boxplot(aes(x = Rsquared, y = model, fill=model)) -> plotres_cubist #and plot the box plot

plot_res_cubist<- plotres_cubist + scale_y_discrete(limits = c("cubist1", "cubist2"))+
    scale_fill_viridis(discrete = TRUE) + theme_bw()  + 
	theme(text=element_text(size=13, family="Palatino"),
	axis.title.y=element_blank())+theme( legend.position="none") ## kesimpulan:

plot_res_cubist #podo

preds_CUBIST_N_All <- predict(tuned_cubist_N_All1, test_N_all)

modelEval_CUBIST_N_All <- cbind(test_N_all$Total_N, preds_CUBIST_N_All)
colnames(modelEval_CUBIST_N_All) <- c('Actual', 'Predicted')
modelEval_CUBIST_N_All <- as.data.frame(modelEval_CUBIST_N_All)

mse_CUBIST_N_All <- mean((modelEval_CUBIST_N_All$Actual - modelEval_CUBIST_N_All$Predicted)^2)
rmse_CUBIST_N_All <- sqrt(mse_CUBIST_N_All)
rmse_CUBIST_N_All

mae_Cubist_N_All <- ModelMetrics::mae(y_test_N_all, preds_CUBIST_N_All)
mae_Cubist_N_All

bias_Cubist_N_All <- Metrics::bias(y_test_N_all, preds_CUBIST_N_All)
bias_Cubist_N_All


## Random Forest

#try ranger first to grasp initial hyperparameterisation

hyper_grid_RF_N_all <- expand.grid(
  mtry       = seq(2, 10, by = 1),
  node_size  = seq(20, 30, by = 1),
  num.trees	 = seq(50, 1000, by = 50),
  OOB_RMSE   = 0
)

for(i in 1:nrow(hyper_grid_RF_N_all)) {
  
  # train model
  model_rf_N_all <- ranger(
    formula         = Total_N ~., 
    data            = boots_N, 
    num.trees       = hyper_grid_RF_N_all$num.trees[i],
    mtry            = hyper_grid_RF_N_all$mtry[i],
    min.node.size   = hyper_grid_RF_N_all$node_size[i],
    sample.fraction = .70,
    seed            = 42
  )
  
  # add OOB error to grid
  hyper_grid_RF_N_all$OOB_RMSE[i] <- sqrt(model_rf_N_all$prediction.error)
}

hyper_grid_RF_N_all %>% 
  dplyr::arrange(OOB_RMSE) %>%  
  arrange(OOB_RMSE) %>%
		top_n(-10, wt = OOB_RMSE)



# use aforementioned tuning parameterisation to supply caret

# tuning model 1
tunegrid_N1 <- expand.grid(.mtry = c(8:11)) 

set.seed(123)
RF_N_caret <- train(
			x = x_train_N_all,
			y = y_train_N_all,
                   method = 'rf',
                   metric = 'RMSE',
                   tuneGrid = tunegrid_N1, 
				   nodesize = 20,
					ntree = 900,
                   trControl = trainControl(method = "repeatedcv", number = 5, repeats=10))

print(RF_N_caret)


# tuning model 2

tunegrid_N2 <- expand.grid(.mtry = c(8:11)) 

RF_N_caret2 <- train(
					x = x_train_N_all,
					y = y_train_N_all,
					method = "rf", 
					trControl = trainControl(method = "repeatedcv", number = 5, repeats=10), 
					metric= "RMSE",
					verbose = FALSE, 
					tuneGrid = tunegrid_N2,
					n.trees = c(4:50)*100) ## tweaking num.of.trees inside
 
print(RF_N_caret2) 

RF_N_caret2$finalModel$ntree

# RF model comparison
res_RF <- resamples(list(RF1 = RF_N_caret, RF2 = RF_N_caret2))
summary(res_RF)
	
scales_RF <- list(x=list(relation="free"), y=list(relation="free"))

bwplot(res_RF, scales=scales_RF)

ggplot(res_RF, scales=scales_RF)

res_RF$values %>% #extract the values
  select(1, ends_with("Rsquared")) %>% #select the first column and all columns with a name ending with "Rsquared"
  gather(model, Rsquared, -1) %>% #convert to long table
  mutate(model = sub("~Rsquared", "", model)) %>% #leave just the model names
  ggplot()+ #call ggplot
  geom_boxplot(aes(x = Rsquared, y = model, fill=model)) -> plotres_RF #and plot the box plot

plot_res_RF<- plotres_RF + scale_y_discrete(limits = c("RF1", "RF2"))+
    scale_fill_viridis(discrete = TRUE) + theme_bw()  + 
	theme(text=element_text(size=13, family="Palatino"),
	axis.title.y=element_blank())+theme( legend.position="none") 

plot_res_RF ## final to model 2


preds_RF_N_All_Final <- predict(RF_N_caret2, test_N_all)

modelEval_RF_N_All <- cbind(test_N_all$Total_N, preds_RF_N_All_Final)
colnames(modelEval_RF_N_All) <- c('Actual', 'Predicted')
modelEval_RF_N_All <- as.data.frame(modelEval_RF_N_All)

mse_RF_N_All <- mean((modelEval_RF_N_All$Actual - modelEval_RF_N_All$Predicted)^2)
rmse_RF_N_All <- sqrt(mse_RF_N_All)
rmse_RF_N_All


mae_rf_N_All <- ModelMetrics::mae(y_test_N_all, preds_RF_N_All_Final)
mae_rf_N_All


bias_rf_N_All <- Metrics::bias(y_test_N_all, preds_RF_N_All_Final)
bias_rf_N_All


## Gradient Boosting Machine

#fixing minobsinnode
grid_gbm <-expand.grid(n.trees = seq(50, 1000, by = 50),
			interaction.depth = c(1, 3, 5), 
			shrinkage = c(.01, .1, .3, .5),
			n.minobsinnode = c(3, 5, 7))
			

set.seed(1234)
gbm_N_All_caret1 <- train(
	x 		= x_train_N_all,
	y 		= y_train_N_all,
	method 	= "gbm",
	metric 	= "RMSE",
	trControl = trainControl(method = "repeatedcv", number = 5, repeats=10),
	tuneGrid = grid_gbm,
	verbose = FALSE
)

gbm_N_All_caret1$bestTune

#evaluate
ggplot(gbm_N_All_caret1) 
# RMSE is stabilised in higher shrinkage+tress (>250), 
# seemingly no difference in varying int.depth and n.minobsinnode

#try adding more trees, eliminate low shrinkage
grid_gbm2 <-expand.grid(n.trees = seq(500, 5000, by = 500),
			interaction.depth = c(3, 5, 7), 
			shrinkage = c(0.3, 0.5),
			n.minobsinnode = c(3,5))
			

set.seed(1234)
gbm_N_All_caret2 <- train(
	x 		= x_train_N_all,
	y 		= y_train_N_all,
	method 	= "gbm",
	metric 	= "RMSE",
	trControl = trainControl(method = "repeatedcv", number = 5, repeats=10),
	tuneGrid = grid_gbm2,
	verbose = FALSE
)

gbm_N_All_caret2$bestTune

#evaluate
ggplot(gbm_N_All_caret2)
# RMSE is significantly lower at shrinkage = 0.3, n.minobsinnode = 3, and int.depth = 3
# RMSE is stabilised at tree >1000


grid_gbm3 <-expand.grid(n.trees = seq(1000, 5000, by = 1000),
			interaction.depth = 3, 
			shrinkage = 0.3,
			n.minobsinnode = 3)
			

set.seed(1234)
gbm_N_All_caret3 <- train(
	x 		= x_train_N_all,
	y 		= y_train_N_all,
	method 	= "gbm",
	metric 	= "RMSE",
	trControl = trainControl(method = "repeatedcv", number = 5, repeats=10),
	tuneGrid = grid_gbm3,
	verbose = FALSE
)

gbm_N_All_caret3$bestTune

#evaluate
ggplot(gbm_N_All_caret3)
# cutoff ntrees for RMSE = 2000

#try adding more trees (considering our high computational capability)
grid_gbm4 <-expand.grid(n.trees = seq(2000, 10000, by = 2000),
			interaction.depth = 3, 
			shrinkage = 0.3,
			n.minobsinnode = 3)
			

set.seed(1234)
gbm_N_All_caret4 <- train(
	x 		= x_train_N_all,
	y 		= y_train_N_all,
	method 	= "gbm",
	metric 	= "RMSE",
	trControl = trainControl(method = "repeatedcv", number = 5, repeats=10),
	tuneGrid = grid_gbm4,
	verbose = FALSE
)

gbm_N_All_caret4$bestTune

#evaluate
ggplot(gbm_N_All_caret4)
# new cutoff ntrees for RMSE = 4000,
# adding more tress than 6000 did not significantly lower RMSE


#try to adjust ntrees
grid_gbm5 <-expand.grid(n.trees = seq(4000, 8000, by = 1000),
			interaction.depth = 3, 
			shrinkage = 0.3,
			n.minobsinnode = 3)
			

set.seed(1234)
gbm_N_All_caret5 <- train(
	x 		= x_train_N_all,
	y 		= y_train_N_all,
	method 	= "gbm",
	metric 	= "RMSE",
	trControl = trainControl(method = "repeatedcv", number = 5, repeats=10),
	tuneGrid = grid_gbm5,
	verbose = FALSE
)

gbm_N_All_caret5$bestTune


#evaluate
ggplot(gbm_N_All_caret5)
# new cutoff ntrees for RMSE = 5000,
# adding more tress than 5000 did not significantly lower RMSE


res_GBM <- resamples(list(GBM1 = gbm_N_All_caret1, GBM2 = gbm_N_All_caret2, GBM3 = gbm_N_All_caret3,GBM4 = gbm_N_All_caret4,GBM5 = gbm_N_All_caret5)) #, GBM3 = gbm_N_All_caret3, GBM4 = gbm_N_All_caret4))
	
scales2 <- list(x=list(relation="free"), y=list(relation="free"))
bwplot(res_GBM, scales=scales2)

ggplot(res_GBM, scales=scales2)

res_GBM$values %>% #extract the values
  select(1, ends_with("Rsquared")) %>% #select the first column and all columns with a name ending with "Rsquared"
  gather(model, Rsquared, -1) %>% #convert to long table
  mutate(model = sub("~Rsquared", "", model)) %>% #leave just the model names
  ggplot()+ #call ggplot
  geom_boxplot(aes(x = Rsquared, y = model, fill=model)) -> plotres_GBM #and plot the box plot

plot_res_GBM <- plotres_GBM + scale_y_discrete(limits = c("GBM1", "GBM2", "GBM3", "GBM4", "GBM5"))+ ##,"GBM3", "GBM4"))+
    scale_fill_viridis(discrete = TRUE) + theme_bw()  + 
	theme(text=element_text(size=13, family="Palatino"),
	axis.title.y=element_blank())+theme( legend.position="none") ##GBM2 unstable, high deviation, no significant differences GBM3-5

plot_res_GBM

#Final Model GBM

grid_gbm_final <-expand.grid(n.trees = seq(4000, 8000, by = 2000),
			interaction.depth = 3, 
			shrinkage = 0.3,
			n.minobsinnode = 3)
			

set.seed(1234)
gbm_N_All_caret_final <- train(
	x 		= x_train_N_all,
	y 		= y_train_N_all,
	method 	= "gbm",
	metric 	= "RMSE",
	trControl = trainControl(method = "repeatedcv", number = 5, repeats=10),
	tuneGrid = grid_gbm_final,
	verbose = FALSE
)

gbm_N_All_caret_final$bestTune


preds_GBM_N_All_caret1 <- predict(gbm_N_All_caret_final, test_N_all)

modelEval_GBM_N_All_caret1 <- cbind(test_N_all$Total_N, preds_GBM_N_All_caret1)
colnames(modelEval_GBM_N_All_caret1) <- c('Actual', 'Predicted')
modelEval_GBM_N_All_caret1 <- as.data.frame(modelEval_GBM_N_All_caret1)

mse_GBM_N_All_caret1 <- mean((modelEval_GBM_N_All_caret1$Actual - modelEval_GBM_N_All_caret1$Predicted)^2)
rmse_GBM_N_All_caret1 <- sqrt(mse_GBM_N_All_caret1)
rmse_GBM_N_All_caret1


mae_gbm_N_All <- ModelMetrics::mae(y_test_N_all, preds_GBM_N_All_caret1)
mae_gbm_N_All


bias_gbm_N_All <- Metrics::bias(y_test_N_all, preds_GBM_N_All_caret1)
bias_gbm_N_All



## extreeme gradient boosting


# Fixing nround, learning rate (eta), max tree depth
xgb_trc_N_All = trainControl(
  method = "repeatedcv",
  number = 5,  
  #repeats = 10, ## dihilangkan buat menambah kecepatan
  allowParallel = TRUE,
  verboseIter = FALSE,
  returnData = FALSE
)


nrounds <- 1000
xgbGrid_N_All <- expand.grid(
  nrounds = seq(from = 200, to = nrounds, by = 50),
  eta = c(0.025, 0.05, 0.1, 0.3),
  max_depth = c(2, 3, 4, 5, 6),
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)


set.seed(42) 
xgb_N_All = train(
	x 		= x_train_N_all,
	y 		= y_train_N_all, 
  trControl = xgb_trc_N_All,
  tuneGrid = xgbGrid_N_All,
  method = "xgbTree",
  metric = 'RMSE',
  verbosity = 0
)

xgb_N_All$bestTune


#evaluate
ggplot(xgb_N_All )
# eta = 0.3 scored significantly lower RMSE, stabilised at all parameterisations (<0.03), eventhough setting eta=0.1 resulted in lowest RMSE
# max_depth>4 yielded lower RMSE
#nrounds > 500 did not signficantly lower RMSE


# re-evaluate eta,max_depth, and nrounds,
nrounds <- 600
xgbGrid_N_All2 <- expand.grid(
  nrounds = seq(from = 300, to = nrounds, by = 50),
  eta = c(0.1, 0.3),
  max_depth = c(4, 5, 6),
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

set.seed(42) 
xgb_N_All2 = train(
	x 		= x_train_N_all,
	y 		= y_train_N_all, 
  trControl = xgb_trc_N_All,
  tuneGrid = xgbGrid_N_All2,
  method = "xgbTree",
  metric = 'RMSE',
  verbosity = 0
)

xgb_N_All2$bestTune


#evaluate
ggplot(xgb_N_All2 )
#nrounds = 350, eta = 0.1, and max_depth=4 had lowest RMSE
#


# Fixing colsample bytree, min_child_weight and subsampling (or column and row sampling = subspace sampling), 
xgb_trc_N_All = trainControl(
  method = "repeatedcv",
  number = 5,  
  #repeats = 10, ## dihilangkan buat menambah kecepatan
  allowParallel = TRUE,
  verboseIter = FALSE,
  returnData = FALSE
)

nrounds <- 500
xgbGrid_N_All3 <- expand.grid(
  nrounds = seq(from = 350, to = nrounds, by = 25),
  eta = c(0.1),
  max_depth = c(4),
  gamma = 0,
  colsample_bytree = c(0.4, 0.6, 0.8, 1.0),
  min_child_weight = c(1,2,3),
  subsample = c(0.5, 0.75, 1.0)
)

set.seed(42) 
xgb_N_All3 = train(
	x 		= x_train_N_all,
	y 		= y_train_N_all, 
  trControl = xgb_trc_N_All,
  tuneGrid = xgbGrid_N_All3,
  method = "xgbTree",
  metric = 'RMSE',
  verbosity = 0
)

xgb_N_All3$bestTune
		
#evaluate
ggplot(xgb_N_All3 )
# subsample = 0.5, colsample_bytree = 0.8, and min_child_weight = 2 yielded lowest RMSE
# nrounds is required to be increase (>500)
# 



# adding higher nrounds, evaluate gamma
xgb_trc_N_All = trainControl(
  method = "repeatedcv",
  number = 5,  
  repeats = 10, 
  allowParallel = TRUE,
  verboseIter = FALSE,
  returnData = FALSE
)

nrounds <- 1000
xgbGrid_N_All4 <- expand.grid(
  nrounds = seq(from = 400, to = nrounds, by = 100),
  eta = c(0.1),
  max_depth = c(4),
  gamma = c(0,1),
  colsample_bytree = c(0.8),
  min_child_weight = c(2),
  subsample = c( 0.5)
)


set.seed(42) 
xgb_N_All4 = train(
	x 		= x_train_N_all,
	y 		= y_train_N_all, 
  trControl = xgb_trc_N_All,
  tuneGrid = xgbGrid_N_All4,
  method = "xgbTree",
  metric = 'RMSE',
  verbosity = 0
)

xgb_N_All4$bestTune

#evaluate
ggplot(xgb_N_All4 )
# lowest RMSE is attained at gamma=0
# nrounds need to be increase

# final XGB model
xgb_trc_N_All = trainControl(
  method = "repeatedcv",
  number = 5,  
  repeats = 10, 
  allowParallel = TRUE,
  verboseIter = FALSE,
  returnData = FALSE
)

nrounds <- 10000
xgbGrid_N_All5 <- expand.grid(
  nrounds = seq(from = 1000, to = nrounds, by = 1000),
  eta = c(0.1),
  max_depth = c(4),
  gamma = c(0),
  colsample_bytree = c(0.8),
  min_child_weight = c(2),
  subsample = c( 0.5)
)


set.seed(42) 
xgb_N_All5 = train(
	x 		= x_train_N_all,
	y 		= y_train_N_all, 
  trControl = xgb_trc_N_All,
  tuneGrid = xgbGrid_N_All5,
  method = "xgbTree",
  metric = 'RMSE',
  verbosity = 0
)

xgb_N_All5$bestTune

#evaluate (again -_-)

ggplot(xgb_N_All5 )


preds_XGB_N_All_Final <- predict(xgb_N_All, as.matrix(x_test_N_all))

modelEval_XGB_N_All <- cbind(y_test_N_all, preds_XGB_N_All_Final)
colnames(modelEval_XGB_N_All) <- c('Actual', 'Predicted')
modelEval_XGB_N_All <- as.data.frame(modelEval_XGB_N_All)

mse_XGB_N_All <- mean((modelEval_XGB_N_All$Actual - modelEval_XGB_N_All$Predicted)^2)
rmse_XGB_N_All <- sqrt(mse_XGB_N_All)
rmse_XGB_N_All

mae_XGB_N_All <- ModelMetrics::mae(y_test_N_all, preds_XGB_N_All_Final)
mae_XGB_N_All

bias_XGB_N_All <- Metrics::bias(y_test_N_all, preds_XGB_N_All_Final)
bias_XGB_N_All


# visualize model agreement
# 1 by calibration (internally computed k-folds CV)

models_compare_N <- resamples(list(LM=MLR_N_All, LogGLM=tuned_log_N_All, MARS=tuned_mars_N_All, 
									CUBIST=tuned_cubist_N_All1, RF=RF_N_caret2, GBM=gbm_N_All_caret5, XGB=xgb_N_All5 ))

# Summary of the models' performances

summary(models_compare_N)

scales <- list(x=list(relation="free"), y=list(relation="free"))
bwplot(models_compare_N, scales=scales)

ggplot(models_compare_N, scales=scales)

models_compare_N$values %>% #extract the values
  select(1, ends_with("RMSE")) %>% #select the first column and all columns with a name ending with "RMSE"
  gather(model, RMSE, -1) %>% #convert to long table
  mutate(model = sub("~RMSE", "", model)) %>% #leave just the model names
  ggplot()+ #call ggplot
  geom_boxplot(aes(x = RMSE, y = model, fill=model)) -> plotN1 #and plot the box plot

plotN1_fix <- plotN1 + scale_y_discrete(limits = c("LM", "LogGLM", "MARS", "CUBIST", "RF", "GBM", "XGB"))+
    scale_colour_brewer(palette = "Oranges") + theme_bw()  + 
	theme(text=element_text(size=13, family="Palatino"),
	axis.title.y=element_blank())+theme( legend.position="none")

models_compare_N$values %>% #extract the values
  select(1, ends_with("MAE")) %>% #select the first column and all columns with a name ending with "MAE"
  gather(model, MAE, -1) %>% #convert to long table
  mutate(model = sub("~MAE", "", model)) %>% #leave just the model names
  ggplot()+ #call ggplot
  geom_boxplot(aes(x = MAE, y = model, fill=model)) -> plotN2 #and plot the box plot

plotN2_fix <- plotN2 + scale_y_discrete(limits = c("LM", "LogGLM", "MARS", "CUBIST", "RF", "GBM", "XGB"))+
    scale_colour_brewer(palette = "Oranges")  + theme_bw()  + 
	theme(text=element_text(size=13, family="Palatino"),
	axis.title.y=element_blank(),
	axis.text.y=element_blank())+theme( legend.position="none")

models_compare_N$values %>% #extract the values
  select(1, ends_with("Rsquared")) %>% #select the first column and all columns with a name ending with "Rsquared"
  gather(model, Rsquared, -1) %>% #convert to long table
  mutate(model = sub("~Rsquared", "", model)) %>% #leave just the model names
  ggplot()+ #call ggplot
  geom_boxplot(aes(x = Rsquared, y = model, fill=model)) -> plotN3 #and plot the box plot

plotN3_fix <- plotN3 + scale_y_discrete(limits = c("LM", "LogGLM", "MARS", "CUBIST", "RF", "GBM", "XGB"))+
    scale_colour_brewer(palette = "Oranges")  + theme_bw() + 
	theme(text=element_text(size=13, family="Palatino"),
	axis.title.y=element_blank(),
	axis.text.y=element_blank())+theme( legend.position="none")


plotN1_fix+plotN2_fix+plotN3_fix


#performance by validation method, gather all metrices

# 2 by validation (externally by the 30% validation data)

									
preds_MLR_N_All1 <- predict(MLR_N_All, test_N_all)
preds_LOG_GLM_N_All1 <- predict(tuned_log_N_All, test_N_all)
preds_MARS_N_All1 <- predict(tuned_mars_N_All, test_N_all)
preds_CUBIST_N_All1 <- predict(tuned_cubist_N_All1, test_N_all)
preds_RF_N_All_Final1 <- predict(RF_N_caret2, test_N_all)
preds_GBM_N_All1 <- predict(gbm_N_All_caret1, test_N_all)
preds_XGB_N_All_Final1 <- predict(xgb_N_All, as.matrix(x_test_N_all))


MLR_df_p <- data.frame(	Rsq = R2(preds_MLR_N_All1, y_test_N_all),  
						RMSE = RMSE(preds_MLR_N_All1, y_test_N_all),  
						MAE = MAE(preds_MLR_N_All1, y_test_N_all),
						BIAS = Metrics::bias(y_test_N_all, preds_MLR_N_All1)) %>% add_column(Model="MLR")


LogGLM_df_p <- data.frame(	Rsq = R2(preds_LOG_GLM_N_All1, y_test_N_all),  
						RMSE = RMSE(preds_LOG_GLM_N_All1, y_test_N_all),  
						MAE = MAE(preds_LOG_GLM_N_All1, y_test_N_all),
						BIAS = Metrics::bias(y_test_N_all, preds_LOG_GLM_N_All1))%>% add_column(Model="LogGLM")

MARS_df_p <- data.frame(Rsq = R2(preds_MARS_N_All1, y_test_N_all),  
						RMSE = RMSE(preds_MARS_N_All1, y_test_N_all),  
						MAE = MAE(preds_MARS_N_All1, y_test_N_all),
						BIAS = Metrics::bias(y_test_N_all, preds_MARS_N_All1))%>% add_column(Model="MARS")

names(MARS_df_p)[names(MARS_df_p) == 'y'] <- 'Rsq'

CUBIST_df_p <- data.frame(	Rsq = R2(preds_CUBIST_N_All1, y_test_N_all),  
						RMSE = RMSE(preds_CUBIST_N_All1, y_test_N_all),  
						MAE = MAE(preds_CUBIST_N_All1, y_test_N_all),
						BIAS = Metrics::bias(y_test_N_all, preds_CUBIST_N_All1))%>% add_column(Model="CUBIST")

RF_df_p <- data.frame(	Rsq = R2(preds_RF_N_All_Final1, y_test_N_all),  
						RMSE = RMSE(preds_RF_N_All_Final1, y_test_N_all),  
						MAE = MAE(preds_RF_N_All_Final1, y_test_N_all),
						BIAS = Metrics::bias(y_test_N_all, preds_RF_N_All_Final1))%>% add_column(Model="RF")
						
GBM_df_p <- data.frame(	Rsq = R2(preds_GBM_N_All1, y_test_N_all),  
						RMSE = RMSE(preds_GBM_N_All1, y_test_N_all),  
						MAE = MAE(preds_GBM_N_All1, y_test_N_all),
						BIAS = Metrics::bias(y_test_N_all, preds_GBM_N_All1))	%>% add_column(Model="GBM")					
						
XGB_df_p <- data.frame(	Rsq = R2(preds_XGB_N_All_Final1, y_test_N_all),  
						RMSE = RMSE(preds_XGB_N_All_Final1, y_test_N_all),  
						MAE = MAE(preds_XGB_N_All_Final1, y_test_N_all),
						BIAS = Metrics::bias(y_test_N_all, preds_XGB_N_All_Final1))	%>% add_column(Model="XGB")					
						

merged_all_model <- bind_rows(MLR_df_p, LogGLM_df_p,MARS_df_p,CUBIST_df_p, RF_df_p,GBM_df_p, XGB_df_p)
merged_all_model 

## Explaining ML with fastshap
# set Monte Carlo Simulation to 1000

shap_mlr <- fastshap::explain(
  MLR_N_All, 
  X = X_df_train,
  nsim = 1000, 
  pred_wrapper = predict,
  shap_only = FALSE,
  adjust = TRUE
)

	
shap_LogGLM <- fastshap::explain(
  tuned_log_N_All, 
  X = X_df_train,
  nsim = 1000, 
  pred_wrapper = predict,
  shap_only = FALSE,
  adjust = TRUE
)


#fail
X_df_train_mars <- data.frame(subset(X_df_train, select = Depth) %>% as.matrix() )
shap_MARS <- fastshap::explain(
  tuned_mars_N_All, 
  X = X_df_train_mars,
  nsim = 1000, 
  pred_wrapper = predict,
  shap_only = FALSE,
  adjust = TRUE
)
sv_MARS <- shapviz(shap_MARS)
	

shap_Cubist <- fastshap::explain(
  tuned_cubist_N_All1, 
  X = X_df_train,
  nsim = 1000, 
  pred_wrapper = predict,
  shap_only = FALSE,
  adjust = TRUE
)


shap_RF <- fastshap::explain(
  RF_N_All_Final, 
  X = X_df_train,
  nsim = 1000, 
  pred_wrapper = predict,
  shap_only = FALSE,
  adjust = TRUE
)


shap_GBM <- fastshap::explain(
  gbm_N_All_caret1, 
  X = X_df_train,
  nsim = 1000, 
  pred_wrapper = predict,
  shap_only = FALSE,
  adjust = TRUE
)


shap_XGB <- fastshap::explain(
  xgb_N_All, 
  X = X_df_train,
  nsim = 1000, 
  pred_wrapper = predict,
  shap_only = FALSE,
  adjust = TRUE
)


## transfer fastshap to shapviz object

sv_LM <- shapviz(shap_mlr)
sv_LogGLM <- shapviz(shap_LogGLM)
sv_Cubist <- shapviz(shap_Cubist)
sv_RF <- shapviz(shap_RF)
sv_GBM <- shapviz(shap_GBM)
sv_XGB <- shapviz(shap_XGB)


## Waterfall Plot

# esplanation at row_id = 3
sv_LM_wtf <- sv_waterfall(sv_LM, row_id = 3, fill_colors=c('#FF0051', '#008BFB')) +ggtitle("LM, row_id=3")+
							theme(text=element_text(size=14,family="Palatino"),
							axis.title.x=element_blank())
sv_LM_wtf$mapping$linetype <- NA

sv_LogGLM_wtf <- sv_waterfall(sv_LogGLM, row_id = 3, fill_colors=c('#FF0051', '#008BFB')) +ggtitle("LogGLM, row_id=3")+
							theme(text=element_text(size=14,family="Palatino"),
							axis.title.x=element_blank())							
sv_LogGLM_wtf$mapping$linetype <- NA

sv_Cubist_wtf <- sv_waterfall(sv_Cubist, row_id = 3, fill_colors=c('#FF0051', '#008BFB')) +ggtitle("CUBIST row_id=3")+
							theme(text=element_text(size=14,family="Palatino"))
sv_Cubist_wtf$mapping$linetype <- NA

sv_RF_wtf <- sv_waterfall(sv_RF, row_id = 3, fill_colors=c('#FF0051', '#008BFB')) +ggtitle("RF, row_id=3")+
							theme(text=element_text(size=14,family="Palatino"),
							axis.title.x=element_blank())	
sv_RF_wtf$mapping$linetype <- NA

sv_GBM_wtf <- sv_waterfall(sv_GBM, row_id = 3, fill_colors=c('#FF0051', '#008BFB')) +ggtitle("GBM, row_id=3")+
							theme(text=element_text(size=14,family="Palatino"),
							axis.title.x=element_blank())	
sv_GBM_wtf$mapping$linetype <- NA

sv_XGB_wtf <- sv_waterfall(sv_XGB, row_id = 3, fill_colors=c('#FF0051', '#008BFB')) +ggtitle("XGB, row_id=3")+
							theme(text=element_text(size=14,family="Palatino"))	
sv_XGB_wtf$mapping$linetype <- NA


sv_LM_wtf+sv_LogGLM_wtf+sv_Cubist_wtf+sv_RF_wtf+sv_GBM_wtf+sv_XGB_wtf+plot_layout(nrow=3)

#explanation at row_id = 100

sv_LogGLM_wtf2 <- sv_waterfall(sv_LogGLM, row_id = 100, fill_colors=c('#FF0051', '#008BFB')) +ggtitle("LogGLM, row_id=100")+
							theme(text=element_text(size=14,family="Palatino"),
							axis.title.x=element_blank())
														
sv_LogGLM_wtf2$mapping$linetype <- NA

sv_Cubist_wtf2 <- sv_waterfall(sv_Cubist, row_id = 100, fill_colors=c('#FF0051', '#008BFB')) +ggtitle("Cubist, row_id=100")+
							theme(text=element_text(size=14,family="Palatino"))	
sv_Cubist_wtf2$mapping$linetype <- NA

sv_GBM_wtf2 <- sv_waterfall(sv_GBM, row_id = 100, fill_colors=c('#FF0051', '#008BFB')) +ggtitle("GBM, row_id=100")+
							theme(text=element_text(size=14,family="Palatino"),
							axis.title.x=element_blank())	
sv_GBM_wtf2$mapping$linetype <- NA

sv_LogGLM_wtf+sv_LogGLM_wtf2+
sv_GBM_wtf+sv_GBM_wtf2+
sv_Cubist_wtf+sv_Cubist_wtf2+
plot_layout(ncol=2)


## Beeswarm Plot

bee_LM <- sv_importance(sv_LM, kind = "beeswarm", show_numbers = FALSE, color_bar_title=NULL)+theme_bw()+ 
							 ggtitle("LM")+
							theme(axis.text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.title.x=element_blank(),
							axis.text.y=element_blank())
							
bee_LogGLM <- sv_importance(sv_LogGLM, kind = "beeswarm", show_numbers = FALSE)+
							theme_bw()+ 
							 ggtitle("LogGLM")+
							theme(axis.text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.title.x=element_blank(),
							axis.text.y=element_blank())
							
bee_Cubist <- sv_importance(sv_Cubist, kind = "beeswarm", show_numbers = FALSE)+
							theme_bw()+ 
							 ggtitle("Cubist") +
							theme(axis.text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.text.y=element_blank()	)						

bee_RF <- sv_importance(sv_RF, kind = "beeswarm", show_numbers = FALSE,  color_bar_title=NULL)+
							theme_bw()+ 
							 ggtitle("RF") +
							theme(axis.text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.title.x=element_blank(),
							axis.text.y=element_blank()	)
							
bee_GBM <- sv_importance(sv_GBM, kind = "beeswarm", show_numbers = FALSE)+
							theme_bw()+ 
							 ggtitle("GBM")+
							theme(axis.text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.text.y=element_blank(),
							axis.title.x=element_blank())
							
bee_XGB <- sv_importance(sv_XGB, kind = "beeswarm", show_numbers = FALSE)+
							theme_bw()+ 
							 ggtitle("XGB")+
							theme(axis.text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.text.y=element_blank())

bee_LM+bee_LogGLM+bee_RF+bee_Cubist+bee_GBM+bee_XGB+plot_layout(nrow=3)


bee_LogGLM+bee_GBM

#importance (mean)

svMean_LM <- sv_importance(sv_LM, kind = "bar", show_numbers = FALSE, color_bar_title=NULL, fill="#674ea7")+theme_bw()+ 
							 ggtitle("LM")+ 
							theme(axis.text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.title.x=element_blank())
							
svMean_LogGLM <- sv_importance(sv_LogGLM, kind = "bar", show_numbers = FALSE,  color_bar_title=NULL, fill="#674ea7")+
							theme_bw()+ 
							 ggtitle("LogGLM")+
							theme(axis.text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.title.x=element_blank(),
							axis.title.y=element_blank())							

svMean_Cubist <- sv_importance(sv_Cubist, kind = "bar", show_numbers = FALSE,  color_bar_title=NULL, fill="#674ea7")+
							theme_bw()+ 
							 ggtitle("Cubist") +
							theme(axis.text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"))
							
svMean_RF <- sv_importance(sv_RF, kind = "bar", show_numbers = FALSE,  color_bar_title=NULL, fill="#674ea7")+
							theme_bw()+ 
							 ggtitle("RF") +
							theme(axis.text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.title.x=element_blank())
							
svMean_GBM <- sv_importance(sv_GBM, kind = "bar", show_numbers = FALSE, , fill="#674ea7")+
							theme_bw()+ 
							 ggtitle("GBM")+
							theme(axis.text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.title.y=element_blank(),
							axis.title.x=element_blank())
							
svMean_XGB <- sv_importance(sv_XGB, kind = "bar", show_numbers = FALSE, , fill="#674ea7")+
							theme_bw()+ 
							 ggtitle("XGB")+
							theme(axis.text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.title.y=element_blank())

svMean_LM+svMean_LogGLM+svMean_RF+svMean_Cubist+svMean_GBM+svMean_XGB+plot_layout(nrow=3)


svMean_LogGLM+bee_LogGLM+svMean_GBM+bee_GBM+
svMean_Cubist+bee_Cubist+plot_layout(nrow=3)


## dependence plot


dep_mlr_depth <- sv_dependence(sv_LM, v = "Depth")+
						theme_bw()+ 
							 ggtitle("LM")+
							 labs(x = " Depth (cm)")+
							theme(text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"))+
							scale_color_continuous(guide = guide_colorbar(
							ticks = FALSE, barheight = 10), type = "viridis")
							
dep_LogGLM_depth <- sv_dependence(sv_LogGLM, v = "Depth")+
						theme_bw()+
							 ggtitle("LogGLM")+
							 labs(x = " Depth (cm)")+
							theme(text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.title.y=element_text(size = 13 ,family="Palatino"))+
							scale_color_continuous(guide = guide_colorbar(
							ticks = FALSE, barheight = 10), type = "viridis") 
							
dep_RF_depth <- sv_dependence(sv_RF, v = "Depth")+
						theme_bw()+
							 ggtitle("RF")+
							 labs(x = " Depth (cm)")+
							theme(text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.title.y=element_blank())+
							scale_color_continuous(guide = guide_colorbar(
							ticks = FALSE, barheight = 10), type = "viridis")

dep_GBM_depth <- sv_dependence(sv_GBM, v = "Depth")+
						theme_bw()+
							 ggtitle("GBM")+
							 labs(x = " Depth (cm)")+
							theme(text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.title.y=element_blank())+
							scale_color_continuous(guide = guide_colorbar(
							ticks = FALSE, barheight = 10), type = "viridis")

dep_XGB_depth <- sv_dependence(sv_XGB, v = "Depth")+
						theme_bw()+
							 ggtitle("XGB")+
							 labs(x = " Depth (cm)")+
							theme(text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.title.y=element_blank())+
							scale_color_continuous(guide = guide_colorbar(
							ticks = FALSE, barheight = 10), type = "viridis")



dep_mlr_depth+dep_LogGLM_depth+dep_RF_depth+dep_GBM_depth+dep_XGB_depth
dep_LogGLM_depth+dep_GBM_depth

dep_mlr_OC <- sv_dependence(sv_LM, v = "Org_C")+
						theme_bw()+ 
							 ggtitle("LM")+
							 labs(x = " Organic C (%)")+
							theme(text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"))+
							scale_color_continuous(guide = guide_colorbar(
							ticks = FALSE, barheight = 10), type = "viridis")				

						
dep_LogGLM_OC <- sv_dependence(sv_LogGLM, v = "Org_C")+
						theme_bw()+
				#			 ggtitle("")+
							 labs(x = " Organic C (%)")+
							theme(text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.title.y=element_text(size = 13 ,family="Palatino"))+
							scale_color_continuous(guide = guide_colorbar(
							ticks = FALSE, barheight = 10), type = "viridis")
							
dep_RF_OC <- sv_dependence(sv_RF, v = "Org_C")+
						theme_bw()+
							 ggtitle("RF")+
							 labs(x = " Organic C (%)")+
							theme(text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.title.y=element_blank())+
							scale_color_continuous(guide = guide_colorbar(
							ticks = FALSE, barheight = 10), type = "viridis")
							
dep_GBM_OC <- sv_dependence(sv_GBM, v = "Org_C")+
						theme_bw()+
				#			 ggtitle("")+
							 labs(x = " Organic C (%)")+
							theme(text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.title.y=element_blank())+
							scale_color_continuous(guide = guide_colorbar(
							ticks = FALSE, barheight = 10), type = "viridis")

dep_XGB_OC <- sv_dependence(sv_XGB, v = "Org_C")+
						theme_bw()+
							 ggtitle("XGB")+
							 labs(x = " Organic C (%)")+
							theme(text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.title.y=element_blank())+
							scale_color_continuous(guide = guide_colorbar(
							ticks = FALSE, barheight = 10), type = "viridis")



dep_mlr_OC+dep_LogGLM_OC+dep_RF_OC+dep_GBM_OC+dep_XGB_OC

dep_LogGLM_depth+dep_GBM_depth+
dep_LogGLM_OC+dep_GBM_OC


dep_mlr_Total_Fe <- sv_dependence(sv_LM, v = "Total_Fe")+
						theme_bw()+ 
							 ggtitle("LM")+
							 labs(x = " Total Fe (mg/kg)")+
							theme(text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"))+
							scale_color_continuous(guide = guide_colorbar(
							ticks = FALSE, barheight = 10), type = "viridis")
							
dep_LogGLM_Total_Fe <- sv_dependence(sv_LogGLM, v = "Total_Fe")+
						theme_bw()+
						#	 ggtitle("")+
							 labs(x = " Total Fe (mg/kg)")+
							theme(text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.title.y=element_text(size = 13 ,family="Palatino"))+
							scale_color_continuous(guide = guide_colorbar(
							ticks = FALSE, barheight = 10), type = "viridis") 
							
dep_RF_Total_Fe <- sv_dependence(sv_RF, v = "Total_Fe")+
						theme_bw()+
							 ggtitle("RF")+
							 labs(x = " Total Fe (mg/kg)")+
							theme(text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.title.y=element_blank())+
							scale_color_continuous(guide = guide_colorbar(
							ticks = FALSE, barheight = 10), type = "viridis")

dep_GBM_Total_Fe <- sv_dependence(sv_GBM, v = "Total_Fe")+
						theme_bw()+
					#		 ggtitle("")+
							 labs(x = " Total Fe (mg/kg)")+
							theme(text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.title.y=element_blank())+
							scale_color_continuous(guide = guide_colorbar(
							ticks = FALSE, barheight = 10), type = "viridis")

dep_XGB_Total_Fe <- sv_dependence(sv_XGB, v = "Total_Fe")+
						theme_bw()+
							 ggtitle("XGB")+
							 labs(x = " Total Fe (mg/kg)")+
							theme(text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.title.y=element_blank())+
							scale_color_continuous(guide = guide_colorbar(
							ticks = FALSE, barheight = 10), type = "viridis")

dep_mlr_Total_Fe+dep_LogGLM_Total_Fe+dep_RF_Total_Fe+dep_GBM_Total_Fe+dep_XGB_Total_Fe

dep_LogGLM_depth+dep_GBM_depth+
dep_LogGLM_OC+dep_GBM_OC +
dep_LogGLM_Total_Fe+dep_GBM_Total_Fe+plot_layout(nrow=3)

dep_mlr_BD <- sv_dependence(sv_LM, v = "BD")+
						theme_bw()+ 
							 ggtitle("LM")+
							 labs(x = " BD (g/cm3)")+
							theme(text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"))+
							scale_color_continuous(guide = guide_colorbar(
							ticks = FALSE, barheight = 10), type = "viridis")
							
dep_LogGLM_BD <- sv_dependence(sv_LogGLM, v = "BD")+
						theme_bw()+
				#			 ggtitle("LogGLM")+
							 labs(x = " BD (g/cm3)")+
							theme(text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.title.y=element_text(size = 13 ,family="Palatino"))+
							scale_color_continuous(guide = guide_colorbar(
							ticks = FALSE, barheight = 10), type = "viridis") 
							
dep_RF_BD <- sv_dependence(sv_RF, v = "BD")+
						theme_bw()+
							 ggtitle("RF")+
							 labs(x = " BD (g/cm3)")+
							theme(text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.title.y=element_blank())+
							scale_color_continuous(guide = guide_colorbar(
							ticks = FALSE, barheight = 10), type = "viridis")

dep_GBM_BD <- sv_dependence(sv_GBM, v = "BD")+
						theme_bw()+
			#				 ggtitle("GBM")+
							 labs(x = " BD (g/cm3)")+
							theme(text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.title.y=element_blank())+
							scale_color_continuous(guide = guide_colorbar(
							ticks = FALSE, barheight = 10), type = "viridis")

dep_XGB_BD <- sv_dependence(sv_XGB, v = "BD")+
						theme_bw()+
							 ggtitle("XGB")+
							 labs(x = " BD (g/cm3)")+
							theme(text = element_text(size = 13 ,family="Palatino"),
							axis.title = element_text(size = 13 ,family="Palatino"),
							axis.title.y=element_blank())+
							scale_color_continuous(guide = guide_colorbar(
							ticks = FALSE, barheight = 10), type = "viridis")

dep_mlr_BD+dep_LogGLM_BD+dep_RF_BD+dep_GBM_BD+dep_XGB_BD

dep_LogGLM_depth+dep_GBM_depth+
dep_LogGLM_OC+dep_GBM_OC +
dep_LogGLM_Total_Fe+dep_GBM_Total_Fe+
dep_LogGLM_BD+dep_GBM_BD +plot_layout(nrow=4)
























